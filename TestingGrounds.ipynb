{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import random\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, Dropout, Lambda, Concatenate\n",
    "\n",
    "# Have to download the stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Get the fasttext model (we are using the largest one they offer [600B tokens])\n",
    "fasttext_model = fasttext.load_model('models/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Useful Function\n",
    "Functions that are continually used throughout this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LEN: 44 EMBEDDING_SHAPE: (300,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Definitions of some sizes in the training set\n",
    "\"\"\"\n",
    "MAX_LEN = 44\n",
    "EMBEDDING_SHAPE = (300,)\n",
    "print('MAX_LEN: ' + str(MAX_LEN), 'EMBEDDING_SHAPE: ' + str(EMBEDDING_SHAPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(df):\n",
    "    max_len = 0\n",
    "    for row in df.itertuples():\n",
    "        if len(row.title_one.split(' ')) > max_len:\n",
    "            max_len = len(row.title_one.split(' '))\n",
    "            \n",
    "        if len(row.title_two.split(' ')) > max_len:\n",
    "            max_len = len(row.title_two.split(' '))\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataframe(df):\n",
    "    for idx in range(len(df)):\n",
    "        print(df.iloc[idx].title_one + '\\n' + df.iloc[idx].title_two)\n",
    "        print('________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_data(pos_df, neg_df):\n",
    "    pos_df.sample(frac=1)\n",
    "    neg_df.sample(frac=1)\n",
    "    final_df = pd.concat([pos_df[:min(len(pos_df), len(neg_df))], neg_df[:min(len(pos_df), len(neg_df))]])\n",
    "    final_df = final_df.sample(frac=1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(phrase):\n",
    "    # Creates the stopwords\n",
    "    to_stop = stopwords.words('english')\n",
    "    punctuation = \"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~ \"\n",
    "    for c in punctuation:\n",
    "        to_stop.append(c)\n",
    "\n",
    "    to_stop.append('null')\n",
    "    \n",
    "    for punc in punctuation:\n",
    "        phrase = phrase.replace(punc, ' ')\n",
    "    \n",
    "    return ' '.join((' '.join([x for x in phrase.split(' ') if x not in to_stop])).split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processsing and Organization\n",
    "Here, all we really want to do is prepare the data for training. This is **only** the data from **Gold Standard** This includes:\n",
    "* Simplifying the original data\n",
    "* Normalizing the data \n",
    "* Balancing the positive and negative examples\n",
    "* Creating the embedding representations that will actually get fed into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing and normalizing the data\n",
    "\"\"\"\n",
    "Essentially, we want to only have three attributes for each training example: title_one, title_two, label\n",
    "For normalization, we are just going to use the nltk stopwords and punctuation\n",
    "\"\"\"\n",
    "\n",
    "def preprocessing(orig_data):\n",
    "    \"\"\"\n",
    "    Normalizes the data by getting rid of stopwords and punctuation\n",
    "    \"\"\"\n",
    "    \n",
    "    # The new names of the columns\n",
    "    column_names = ['title_one', 'title_two', 'label']\n",
    "    # A new dataframe for the data we are going to be creating\n",
    "    norm_data = pd.DataFrame(columns = column_names)\n",
    "    # Iterate over the original dataframe (I know it is slow and there are probably better ways to do it)\n",
    "    iloc_data = orig_data.iloc\n",
    "    for idx in tqdm(range(len(orig_data))):\n",
    "        row = iloc_data[idx]\n",
    "        title_left = remove_stop_words(row.title_left)\n",
    "        title_right = remove_stop_words(row.title_right)\n",
    "        \n",
    "        # Append the newly created row (title_left, title_right, label) to the new dataframe\n",
    "        norm_data = norm_data.append(pd.DataFrame([[title_left, title_right, row.label]], columns=column_names))\n",
    "    \n",
    "    return norm_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_df(df):\n",
    "    \"\"\"\n",
    "    Returns a shuffled dataframe with an equal amount of positive and negative examples\n",
    "    \"\"\"\n",
    "    # Get the positive and negative examples\n",
    "    pos_df = df.loc[df['label'] == 1]\n",
    "    neg_df = df.loc[df['label'] == 0]\n",
    "    \n",
    "    # Shuffle the data\n",
    "    pos_df = pos_df.sample(frac=1)\n",
    "    neg_df = neg_df.sample(frac=1)\n",
    "    \n",
    "    # Concatenate the positive and negative examples and \n",
    "    # make sure there are only as many negative examples as positive examples\n",
    "    final_df = pd.concat([pos_df[:min(len(pos_df), len(neg_df))], neg_df[:min(len(pos_df), len(neg_df))]])\n",
    "    \n",
    "    # Shuffle the final data once again\n",
    "    final_df.sample(frac=1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(df, path):\n",
    "    \"\"\"\n",
    "    Creates and saves a simpler version of the original data that only contains the the two titles and the label.\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_bal_data = create_train_df(preprocessing(df))\n",
    "    \n",
    "    # Save the new normalized and simplified data to a CSV file to load later\n",
    "    norm_bal_data.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "computer_df = pd.read_json('data/train/computers_train_xlarge_normalized.json.gz', compression='gzip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See some of the data. There is clearly a separation between the positive and negative examples\n",
    "computer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the data if the simple and normalized data does not exist\n",
    "computer_data_path = 'data/train/computers_train_bal_shuffle.csv'\n",
    "\n",
    "# If the computer data has not been made yet, make it\n",
    "if not os.path.exists(computer_data_path):\n",
    "    create_training_data(computer_df, computer_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cameras data\n",
    "camera_df = pd.read_json('data/train/cameras_train_xlarge_normalized.json.gz', compression='gzip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the data if the simple and normalized data does not exist\n",
    "camera_data_path = 'data/train/cameras_train_bal_shuffle.csv'\n",
    "\n",
    "# If the computer data has not been made yet, make it\n",
    "if not os.path.exists(camera_data_path):\n",
    "    create_training_data(camera_df, camera_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_computer_df = pd.read_csv('data/train/computers_train_bal_shuffle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_one</th>\n",
       "      <th>title_two</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intel nuc kit nuc5i5ryh i5 5250u boxnuc5i5ryh ...</td>\n",
       "      <td>intel nuc nuc5i5ryh i5 5250u pccomponentes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crucial 4gb 1x4gb 2133mhz ddr4 ecc rdimm 1 2v ...</td>\n",
       "      <td>crucial ddr4 4 gb dimm 288 pin ct4g4rfs8213 se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hdd gold 6tb sata 128mb 3 5 smartphones tu tie...</td>\n",
       "      <td>wd gold 6tb 3 5 7200rpm 128mb cache datacenter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corsair vengeance led 32gb 2 x 16gb ddr4 dram ...</td>\n",
       "      <td>corsair vengeance led 32gb 2x16gb ddr4 pc4 240...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data 75icc60010 cbl 16gb compactflash card 133...</td>\n",
       "      <td>gigaram cf 16gb 120x cbl 50p r21mb w9mb bulk b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19375</th>\n",
       "      <td>seagate st1000lm014 1tb sshd 2 5 sata 3 pc mac...</td>\n",
       "      <td>wd blue pc ssd wds100t1b0a solid state drive 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19376</th>\n",
       "      <td>kingston technology fury 8gb 2133mhz ddr4 hype...</td>\n",
       "      <td>hyperx fury ddr3 8 gb dimm 240 pin hx318c10f s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19377</th>\n",
       "      <td>gigabyte geforce gtx 1060 mini itx oc 6gb gddr5</td>\n",
       "      <td>asus gtx750ti oc 2gd5 geforce gtx 750 ti graph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19378</th>\n",
       "      <td>apple smart keyboard folio case english mptl2l...</td>\n",
       "      <td>apple 12 9 inch ipad pro wi fi 32 gb gold</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19379</th>\n",
       "      <td>mac mini aluminum unibody 2 6ghz dual core i5 ...</td>\n",
       "      <td>apple mac mini 2 8ghz intel core i5 4260u 4gb ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19380 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title_one  \\\n",
       "0      intel nuc kit nuc5i5ryh i5 5250u boxnuc5i5ryh ...   \n",
       "1      crucial 4gb 1x4gb 2133mhz ddr4 ecc rdimm 1 2v ...   \n",
       "2      hdd gold 6tb sata 128mb 3 5 smartphones tu tie...   \n",
       "3      corsair vengeance led 32gb 2 x 16gb ddr4 dram ...   \n",
       "4      data 75icc60010 cbl 16gb compactflash card 133...   \n",
       "...                                                  ...   \n",
       "19375  seagate st1000lm014 1tb sshd 2 5 sata 3 pc mac...   \n",
       "19376  kingston technology fury 8gb 2133mhz ddr4 hype...   \n",
       "19377    gigabyte geforce gtx 1060 mini itx oc 6gb gddr5   \n",
       "19378  apple smart keyboard folio case english mptl2l...   \n",
       "19379  mac mini aluminum unibody 2 6ghz dual core i5 ...   \n",
       "\n",
       "                                               title_two  label  \n",
       "0             intel nuc nuc5i5ryh i5 5250u pccomponentes      1  \n",
       "1      crucial ddr4 4 gb dimm 288 pin ct4g4rfs8213 se...      1  \n",
       "2      wd gold 6tb 3 5 7200rpm 128mb cache datacenter...      1  \n",
       "3      corsair vengeance led 32gb 2x16gb ddr4 pc4 240...      1  \n",
       "4      gigaram cf 16gb 120x cbl 50p r21mb w9mb bulk b...      1  \n",
       "...                                                  ...    ...  \n",
       "19375  wd blue pc ssd wds100t1b0a solid state drive 1...      0  \n",
       "19376  hyperx fury ddr3 8 gb dimm 240 pin hx318c10f s...      0  \n",
       "19377  asus gtx750ti oc 2gd5 geforce gtx 750 ti graph...      0  \n",
       "19378          apple 12 9 inch ipad pro wi fi 32 gb gold      0  \n",
       "19379  apple mac mini 2 8ghz intel core i5 4260u 4gb ...      0  \n",
       "\n",
       "[19380 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_computer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_camera_df = pd.read_csv('data/train/cameras_train_bal_shuffle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_camera_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laptop Data Preprocessing\n",
    "* Normalize the data\n",
    "* Create negative examples that represent when only a couple of attributes of the laptop data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the laptop data\n",
    "laptop_df = pd.read_csv('data/train/laptops.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price_euros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 2.3GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 640</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>1339.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Macbook Air</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1440x900</td>\n",
       "      <td>Intel Core i5 1.8GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics 6000</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>898.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HP</td>\n",
       "      <td>250 G6</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Full HD 1920x1080</td>\n",
       "      <td>Intel Core i5 7200U 2.5GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel HD Graphics 620</td>\n",
       "      <td>No OS</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>575.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>15.4</td>\n",
       "      <td>IPS Panel Retina Display 2880x1800</td>\n",
       "      <td>Intel Core i7 2.7GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>AMD Radeon Pro 455</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>2537.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 3.1GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 650</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>1803.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1316</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Yoga 500-14ISK</td>\n",
       "      <td>2 in 1 Convertible</td>\n",
       "      <td>14.0</td>\n",
       "      <td>IPS Panel Full HD / Touchscreen 1920x1080</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>4GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel HD Graphics 520</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.8kg</td>\n",
       "      <td>638.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1317</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Yoga 900-13ISK</td>\n",
       "      <td>2 in 1 Convertible</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Quad HD+ / Touchscreen 3200x1800</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>Intel HD Graphics 520</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.3kg</td>\n",
       "      <td>1499.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>1318</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>IdeaPad 100S-14IBR</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Celeron Dual Core N3050 1.6GHz</td>\n",
       "      <td>2GB</td>\n",
       "      <td>64GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.5kg</td>\n",
       "      <td>229.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>1319</td>\n",
       "      <td>HP</td>\n",
       "      <td>15-AC110nv (i7-6500U/6GB/1TB/Radeon</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>6GB</td>\n",
       "      <td>1TB HDD</td>\n",
       "      <td>AMD Radeon R5 M330</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>2.19kg</td>\n",
       "      <td>764.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>1320</td>\n",
       "      <td>Asus</td>\n",
       "      <td>X553SA-XX031T (N3050/4GB/500GB/W10)</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Celeron Dual Core N3050 1.6GHz</td>\n",
       "      <td>4GB</td>\n",
       "      <td>500GB HDD</td>\n",
       "      <td>Intel HD Graphics</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>2.2kg</td>\n",
       "      <td>369.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 Company                              Product  \\\n",
       "0              1   Apple                          MacBook Pro   \n",
       "1              2   Apple                          Macbook Air   \n",
       "2              3      HP                               250 G6   \n",
       "3              4   Apple                          MacBook Pro   \n",
       "4              5   Apple                          MacBook Pro   \n",
       "...          ...     ...                                  ...   \n",
       "1298        1316  Lenovo                       Yoga 500-14ISK   \n",
       "1299        1317  Lenovo                       Yoga 900-13ISK   \n",
       "1300        1318  Lenovo                   IdeaPad 100S-14IBR   \n",
       "1301        1319      HP  15-AC110nv (i7-6500U/6GB/1TB/Radeon   \n",
       "1302        1320    Asus  X553SA-XX031T (N3050/4GB/500GB/W10)   \n",
       "\n",
       "                TypeName  Inches                            ScreenResolution  \\\n",
       "0              Ultrabook    13.3          IPS Panel Retina Display 2560x1600   \n",
       "1              Ultrabook    13.3                                    1440x900   \n",
       "2               Notebook    15.6                           Full HD 1920x1080   \n",
       "3              Ultrabook    15.4          IPS Panel Retina Display 2880x1800   \n",
       "4              Ultrabook    13.3          IPS Panel Retina Display 2560x1600   \n",
       "...                  ...     ...                                         ...   \n",
       "1298  2 in 1 Convertible    14.0   IPS Panel Full HD / Touchscreen 1920x1080   \n",
       "1299  2 in 1 Convertible    13.3  IPS Panel Quad HD+ / Touchscreen 3200x1800   \n",
       "1300            Notebook    14.0                                    1366x768   \n",
       "1301            Notebook    15.6                                    1366x768   \n",
       "1302            Notebook    15.6                                    1366x768   \n",
       "\n",
       "                                       Cpu   Ram               Memory  \\\n",
       "0                     Intel Core i5 2.3GHz   8GB            128GB SSD   \n",
       "1                     Intel Core i5 1.8GHz   8GB  128GB Flash Storage   \n",
       "2               Intel Core i5 7200U 2.5GHz   8GB            256GB SSD   \n",
       "3                     Intel Core i7 2.7GHz  16GB            512GB SSD   \n",
       "4                     Intel Core i5 3.1GHz   8GB            256GB SSD   \n",
       "...                                    ...   ...                  ...   \n",
       "1298            Intel Core i7 6500U 2.5GHz   4GB            128GB SSD   \n",
       "1299            Intel Core i7 6500U 2.5GHz  16GB            512GB SSD   \n",
       "1300  Intel Celeron Dual Core N3050 1.6GHz   2GB   64GB Flash Storage   \n",
       "1301            Intel Core i7 6500U 2.5GHz   6GB              1TB HDD   \n",
       "1302  Intel Celeron Dual Core N3050 1.6GHz   4GB            500GB HDD   \n",
       "\n",
       "                               Gpu       OpSys  Weight  Price_euros  \n",
       "0     Intel Iris Plus Graphics 640       macOS  1.37kg      1339.69  \n",
       "1           Intel HD Graphics 6000       macOS  1.34kg       898.94  \n",
       "2            Intel HD Graphics 620       No OS  1.86kg       575.00  \n",
       "3               AMD Radeon Pro 455       macOS  1.83kg      2537.45  \n",
       "4     Intel Iris Plus Graphics 650       macOS  1.37kg      1803.60  \n",
       "...                            ...         ...     ...          ...  \n",
       "1298         Intel HD Graphics 520  Windows 10   1.8kg       638.00  \n",
       "1299         Intel HD Graphics 520  Windows 10   1.3kg      1499.00  \n",
       "1300             Intel HD Graphics  Windows 10   1.5kg       229.00  \n",
       "1301            AMD Radeon R5 M330  Windows 10  2.19kg       764.00  \n",
       "1302             Intel HD Graphics  Windows 10   2.2kg       369.00  \n",
       "\n",
       "[1303 rows x 13 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will be used in order to exchange the different attributes\n",
    "# to create negative examples\n",
    "class Attributes():\n",
    "    company = {'Apple'}\n",
    "    product = {'MacBook Pro'}\n",
    "    inches = {'13.3'}\n",
    "    cpu = {'Intel Core i5 2.3GHz'}\n",
    "    ram = {'4GB'}\n",
    "    memory = {'256GB SSD'}\n",
    "    gpu = {'Intel HD Graphics 520'}\n",
    "    screen = {'1440x900'}\n",
    "    \n",
    "    def get_all_data():\n",
    "        return {\n",
    "            'company': Attributes.company,\n",
    "            'product': Attributes.product,\n",
    "            'inches': Attributes.inches,\n",
    "            'cpu': Attributes.cpu,\n",
    "            'ram': Attributes.ram,\n",
    "            'memory': Attributes.memory,\n",
    "            'gpu': Attributes.gpu,\n",
    "            'screen': Attributes.screen\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attribute sets\n",
    "def create_attribute_sets(df):\n",
    "    Attributes.company.update([row.Company for row in laptop_df[['Company']].itertuples()])\n",
    "    Attributes.product.update([row.Product for row in laptop_df[['Product']].itertuples()])\n",
    "    Attributes.inches.update([str(row.Inches) for row in laptop_df[['Inches']].itertuples()])\n",
    "    Attributes.cpu.update([row.Cpu for row in laptop_df[['Cpu']].itertuples()])\n",
    "    Attributes.ram.update([row.Ram for row in laptop_df[['Ram']].itertuples()])\n",
    "    Attributes.memory.update([row.Memory for row in laptop_df[['Memory']].itertuples()])\n",
    "    Attributes.gpu.update([row.Gpu for row in laptop_df[['Gpu']].itertuples()])\n",
    "    Attributes.screen.update([row.ScreenResolution for row in laptop_df[['ScreenResolution']].itertuples()])\n",
    "\n",
    "create_attribute_sets(laptop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_row(row):\n",
    "    # Note: got rid of everything after the '(' because it has info about the actual specs of the laptop\n",
    "    # so if we change the specs, we need to fix that too\n",
    "    \n",
    "    # Special tags at the end of the amount of inches of the laptop and the RAM to simulate real data\n",
    "    inch_attr = str(row['Inches']) + random.choice([' inch', '', '\"'])\n",
    "    ram_attr = row['Ram'] + random.choice([' ram', ' memory', ''])\n",
    "    \n",
    "    # These are words that commonly come up with laptops\n",
    "    modifiers = ['premium', 'new', 'fast', 'latest model']\n",
    "    add_ins = ['USB 3.0', 'USB 3.1 Type-C', 'USB Type-C', 'Bluetooth', 'WIFI', 'Webcam', 'FP Reader',\n",
    "               'HDMI', '802.11ac', '802.11 ac', 'home', 'flagship', 'business', 'GbE LAN', 'DVD-RW', 'DVD', 'Windows 10']\n",
    "    \n",
    "    cpu_attr = row['Cpu']\n",
    "    if random.choice([0, 1]):\n",
    "        cpu_attr = cpu_attr.split(' ')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'Intel' in cpu_attr:\n",
    "                cpu_attr.remove('Intel')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'Core' in cpu_attr:\n",
    "                cpu_attr.remove('Core')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'AMD' in cpu_attr:\n",
    "                cpu_attr.remove('AMD')\n",
    "    \n",
    "        cpu_attr = ' '.join(cpu_attr)\n",
    "\n",
    "    # Create a list for all the product attributes\n",
    "    order_attrs = [random.choice(modifiers),\n",
    "                   row['Company'],\n",
    "                   row['Product'].split('(')[0],\n",
    "                   row['TypeName'],\n",
    "                   inch_attr,\n",
    "                   row['ScreenResolution'],\n",
    "                   cpu_attr,\n",
    "                   ram_attr,\n",
    "                   row['Memory'],\n",
    "                   row['Gpu']]\n",
    "    \n",
    "    order_attrs = order_attrs + random.sample(add_ins, random.choice([1, 2, 3, 4]))\n",
    "    \n",
    "    # Shuffle the data because in real data, it does not really matter what order the attributes are in\n",
    "    random.shuffle(order_attrs)\n",
    "    \n",
    "    return ' '.join(order_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the negative examples for the laptop data\n",
    "# The laptop_df is the original data, the new_df is the dataframe to append the new data to\n",
    "# and the attributes are the attributes to swap for the new data\n",
    "def create_neg_laptop_data(laptop_df, attributes):\n",
    "    new_column_names = ['title_one', 'title_two', 'label']\n",
    "    negative_df = pd.DataFrame(columns = new_column_names)\n",
    "    for row in tqdm(range(len(laptop_df))):\n",
    "        # Create a copy of the row for the negative example\n",
    "        neg_row = laptop_df.iloc[row]\n",
    "        for attribute_class in attributes:\n",
    "            # Get the row in the laptop_data\n",
    "            orig_row = laptop_df.iloc[row]\n",
    "            \n",
    "            # Get the attribute that we are trying to change\n",
    "            attribute_val = orig_row[attribute_class]\n",
    "            \n",
    "            # Temporarily value for the new value\n",
    "            new_val = attribute_val\n",
    "            \n",
    "            # Make sure we really get a new attribute\n",
    "            while new_val == attribute_val:\n",
    "                new_val = random.sample(Attributes.get_all_data()[attribute_class.lower()], 1)[0]\n",
    "            \n",
    "            # Change the value in the neg_row to the new value\n",
    "            neg_row[attribute_class] = new_val\n",
    "            \n",
    "            # Concatenate and normalize the data\n",
    "            title_one = remove_stop_words(concatenate_row(orig_row).lower())\n",
    "            title_two = remove_stop_words(concatenate_row(neg_row).lower())\n",
    "            \n",
    "            # Append the data to the new df\n",
    "            negative_df = negative_df.append(pd.DataFrame([[title_one, title_two, 0]], columns=new_column_names))\n",
    "    \n",
    "    return negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/1303 [00:00<?, ?it/s]C:\\Users\\Jason\\miniconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1303/1303 [00:17<00:00, 73.64it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_df = create_neg_laptop_data(laptop_df, attributes=['Cpu', 'Memory', 'Ram', 'Inches', 'Product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe(neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the postive examples for the laptop data\n",
    "# The laptop_df is the original data, the new_df is the dataframe to append the new data to\n",
    "# and the attributes are the attributes to swap or delete for the new data\n",
    "def create_pos_laptop_data(laptop_df, rm_attrs, add_attrs):\n",
    "    new_column_names = ['title_one', 'title_two', 'label']\n",
    "    pos_df = pd.DataFrame(columns = new_column_names)\n",
    "    for row in tqdm(range(len(laptop_df))):\n",
    "        # Remove the attribute from the new title\n",
    "        for attr_list in rm_attrs:\n",
    "            # Create a copy of the row for the negative example\n",
    "            new_row = laptop_df.iloc[row]\n",
    "            orig_row = laptop_df.iloc[row]\n",
    "            for attr in attr_list:\n",
    "                new_row[attr] = ''\n",
    "        \n",
    "            title_one = remove_stop_words(concatenate_row(orig_row).lower())\n",
    "            title_two = remove_stop_words(concatenate_row(new_row).lower())\n",
    "\n",
    "            pos_df = pos_df.append(pd.DataFrame([[title_one, title_two, 1]], columns=new_column_names))\n",
    "\n",
    "    return pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/1303 [00:00<?, ?it/s]C:\\Users\\Jason\\miniconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      " 22%|████████████████▍                                                           | 282/1303 [00:03<00:14, 72.25it/s]"
     ]
    }
   ],
   "source": [
    "pos_df = create_pos_laptop_data(laptop_df, rm_attrs = [['Company'], ['TypeName'], ['ScreenResolution'], ['Product'], ['TypeName', 'ScreenResolution']], add_attrs = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_laptop_df = create_final_data(pos_df, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_laptop_df = final_laptop_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCPartPicker Data\n",
    "* Organize the data\n",
    "* Preprocess the data\n",
    "* Create negative and positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_df = pd.read_csv('data/train/pos_ram_titles.csv')\n",
    "cpu_df = pd.read_csv('data/train/pos_cpu_titles.csv')\n",
    "hard_drive_df = pd.read_csv('data/train/pos_hard_drive_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_drive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Unnamed: 0 column and drop any row where it is all NaN\n",
    "def remove_misc(df):\n",
    "    columns = list(df.columns)[1:]\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "    df = df.dropna(how='all')\n",
    "    print(len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_df = remove_misc(ram_df)\n",
    "cpu_df = remove_misc(cpu_df)\n",
    "hard_drive_df = remove_misc(hard_drive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_pcpartpicker_data(df):\n",
    "    columns = list(df.columns)\n",
    "    pos_df = pd.DataFrame(columns=['title_one', 'title_two', 'label'])\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        row = df.iloc()[idx]\n",
    "        titles = []\n",
    "        for col in columns:\n",
    "            if not pd.isnull(row[col]): titles.append(row[col])\n",
    "        if len(titles) > 1:\n",
    "            combs = combinations(titles, 2)\n",
    "            for comb in combs:\n",
    "                comb = list(comb)\n",
    "                comb.append(1)\n",
    "                pos_df = pos_df.append(pd.DataFrame([comb], columns=['title_one', 'title_two', 'label']))\n",
    "    \n",
    "    return pos_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ram_data = generate_pos_pcpartpicker_data(ram_df)\n",
    "\n",
    "pos_cpu_data = generate_pos_pcpartpicker_data(cpu_df)\n",
    "\n",
    "pos_hard_drive_data = generate_pos_pcpartpicker_data(hard_drive_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_pcpartpicker_data(df):\n",
    "    columns = list(df.columns)\n",
    "    neg_df = pd.DataFrame(columns=['title_one', 'title_two', 'label'])\n",
    "    df_list = df.iloc()\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        row = df_list[idx]\n",
    "        for col in columns:\n",
    "            if not pd.isnull(row[col]):\n",
    "                neg_idx = None\n",
    "                while neg_idx == idx or neg_idx is None:\n",
    "                    neg_idx = random.randint(0, len(df) - 1)\n",
    "                \n",
    "                neg_title = None\n",
    "                while neg_title == None or pd.isnull(neg_title):\n",
    "                    neg_title = df_list[neg_idx][random.choice(columns)]\n",
    "                \n",
    "                neg_df = neg_df.append(pd.DataFrame([[row[col], neg_title, 0]], columns=['title_one', 'title_two', 'label']))\n",
    "    \n",
    "    return neg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ram_data = generate_neg_pcpartpicker_data(ram_df)\n",
    "\n",
    "neg_cpu_data = generate_neg_pcpartpicker_data(cpu_df)\n",
    "\n",
    "neg_hard_drive_data = generate_neg_pcpartpicker_data(hard_drive_df)\n",
    "\n",
    "final_ram_data = create_final_data(pos_ram_data, neg_ram_data)\n",
    "\n",
    "final_cpu_data = create_final_data(pos_cpu_data, neg_cpu_data)\n",
    "\n",
    "final_hard_drive_data = create_final_data(pos_hard_drive_data, neg_hard_drive_data)\n",
    "\n",
    "print(len(final_cpu_data), len(final_ram_data), len(final_hard_drive_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Computer Data Generation\n",
    "* Using the PCPartPicker data, we combine computer parts (e.g. CPU, hard drive, RAM, etc.) and create positive and negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Creation Functions\n",
    "Generates the embeddings and saves them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the numpy files of all the training embedddings\n",
    "We will have two numpy files:\n",
    "1. The training/validation/test sets\n",
    "2. The labels\n",
    "\"\"\"\n",
    "\n",
    "def create_embeddings(df):\n",
    "    # Create the numpy arrays for storing the embeddings and labels\n",
    "    total_embeddings = np.zeros(shape=(len(df), 2, MAX_LEN, EMBEDDING_SHAPE[0]))\n",
    "    labels = np.zeros(shape=(len(df)))\n",
    "    \n",
    "    # I know this is a terrible way of doing this, but iterate over the dataframe\n",
    "    # and generate the embeddings to add to the numpy array\n",
    "    for idx, row in enumerate(tqdm(df.itertuples())):\n",
    "        for word_idx, word in enumerate(row.title_one.split()):\n",
    "            total_embeddings[idx, 0, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        for word_idx, word in enumerate(row.title_two.split()):\n",
    "            total_embeddings[idx, 1, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        labels[idx] = row.label\n",
    "        \n",
    "    return total_embeddings, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(df, embeddings_name, labels_name):\n",
    "    \"\"\"\n",
    "    Saves the embeddings given the embeddings file name and labels file name\n",
    "    \"\"\"\n",
    "    if not os.path.exists('data/numpy_data/' + embeddings_name + '.npy'):\n",
    "        embeddings, labels = create_embeddings(df)\n",
    "        with open('data/numpy_data/' + embeddings_name + '.npy', 'wb') as f:\n",
    "            np.save(f, embeddings)\n",
    "\n",
    "        with open('data/numpy_data/' + labels_name + '.npy', 'wb') as f:\n",
    "            np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_labels(embeddings_name, labels_name):\n",
    "    loaded_embeddings = None\n",
    "    labels = None\n",
    "    with open('data/numpy_data/' + embeddings_name + '.npy', 'rb') as f:\n",
    "        loaded_embeddings = np.load(f)\n",
    "        loaded_embeddings = np.transpose(loaded_embeddings, (1, 0, 2, 3))\n",
    "    \n",
    "    with open('data/numpy_data/' + labels_name + '.npy', 'rb') as f:\n",
    "        labels = np.load(f)\n",
    "    \n",
    "    return loaded_embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Embeddings\n",
    "Save the embeddings for the different types of data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate everything\n",
    "total_data = pd.concat([final_computer_df, final_laptop_df, final_hard_drive_data, final_cpu_data, final_ram_data])\n",
    "total_data = total_data.sample(frac=1)\n",
    "MAX_LEN = get_max_len(total_data)\n",
    "save_embeddings(total_data, 'all_embeddings', 'all_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, labels = load_embeddings_and_labels('all_embeddings', 'all_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35078"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (2, 31078, 44, 300)\n",
      "Val shape: (2, 2000, 44, 300)\n",
      "Test shape: (2, 2000, 44, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = embeddings[0, :len(labels) - 4000]\n",
    "X_train2 = embeddings[1, :len(labels) - 4000]\n",
    "X_train = np.stack((X_train1, X_train2))\n",
    "print('Training shape: ' + str(X_train.shape))\n",
    "\n",
    "X_val1 = embeddings[0, len(labels) - 4000:len(labels) - 2000]\n",
    "X_val2 = embeddings[1, len(labels) - 4000:len(labels) - 2000]\n",
    "X_val = np.stack((X_val1, X_val2))\n",
    "print('Val shape: ' + str(X_val.shape))\n",
    "\n",
    "\n",
    "X_test1 = embeddings[0, len(labels) - 2000:]\n",
    "X_test2 = embeddings[1, len(labels) - 2000:]\n",
    "X_test = np.stack((X_test1, X_test2))\n",
    "print('Test shape: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (31078,)\n",
      "Val shape: (2000,)\n",
      "Test shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "Y_train = labels[:len(labels) - 4000]\n",
    "print('Training labels shape:', str(Y_train.shape))\n",
    "\n",
    "Y_val = labels[len(labels) - 4000:len(labels) - 2000]\n",
    "print('Val shape:', str(Y_val.shape))\n",
    "\n",
    "Y_test = labels[len(labels) - 2000:]\n",
    "print('Test shape:', str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = convert_to_one_hot(Y_train.astype(np.int32), 2)\n",
    "Y_val = convert_to_one_hot(Y_val.astype(np.int32), 2)\n",
    "Y_test = convert_to_one_hot(Y_test.astype(np.int32), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Info\n",
    "\n",
    "For the model, we are going to use LSTMs with a Constrastive Loss Function \n",
    "that will also be used to predict whether the two products are the same \n",
    "\n",
    "First, we have to convert the titles to embeddings through FastText before feeding into the LSTM.\n",
    "The embedding part of this model will not be a layer because:\n",
    "* The fasttext model would be time consuming and annoying to get to work with an embedding layer in Keras\n",
    "* The fasttext model is not going to be getting its embeddings optimized, so there is really no point in adding it as an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return tf.square(x - y)\n",
    "\n",
    "def euclidean_dist_out_shape(shapes):\n",
    "    # Both inputs are fed in, so just use one of them and get the first value in the shape\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],)\n",
    "\n",
    "def siamese_network(input_shape):\n",
    "    # Defines our inputs\n",
    "    left_title = Input(input_shape, dtype='float32')\n",
    "    right_title = Input(input_shape, dtype='float32')\n",
    "    \n",
    "    # The LSTM units\n",
    "    model = tf.keras.Sequential(name='siamese_model')\n",
    "    model.add(Bidirectional(LSTM(units=256, name='lstm_1')))\n",
    "    model.add(Dropout(rate=0.6))\n",
    "#     model.add(LSTM(units=128, return_sequences=True, name='lstm_2'))\n",
    "#     model.add(Dropout(rate=0.6))\n",
    "#     model.add(LSTM(units=128, name='lstm_3'))\n",
    "#     model.add(Dropout(rate=0.6))\n",
    "    \n",
    "    # The dense layers\n",
    "    model.add(Dense(units=512, activation='elu', name='dense_1'))\n",
    "    model.add(Dropout(rate=0.6))\n",
    "    model.add(Dense(units=256, activation='elu', name='dense_2'))\n",
    "    \n",
    "    # Forward propagate through the model to generate the encodings\n",
    "    encoded_left_title = model(left_title)\n",
    "    encoded_right_title = model(right_title)\n",
    "\n",
    "    SquareDistanceLayer = Lambda(square_distance)\n",
    "    distance = SquareDistanceLayer([encoded_left_title, encoded_right_title])\n",
    "    \n",
    "    prediction = Dense(units=2, activation='softmax')(distance)\n",
    "    # Create and return the network\n",
    "    siamese_net = tf.keras.Model(inputs=[left_title, right_title], outputs=prediction, name='siamese_network')\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for the constrastive loss, because 0 denotes that they are from the same class\n",
    "# and one denotes they are from a different class, I swaped the (Y) and (1 - Y) terms\n",
    "\n",
    "def constrastive_loss(y_true, y_pred):\n",
    "    margin = 2.0\n",
    "    d = y_pred\n",
    "    d_sqrt = tf.sqrt(d)\n",
    "    #tf.print('\\nY Pred: ', d, 'Shape: ', tf.shape(d))\n",
    "    #tf.print('\\nY True: ', y_true, 'Shape: ', tf.shape(y_true))\n",
    "    \n",
    "    loss = (y_true * d) + ((1 - y_true) * tf.square(tf.maximum(0., margin - d_sqrt)))\n",
    "    \n",
    "    #tf.print('\\n Constrastive Loss: ', loss, 'Shape: ', tf.shape(loss))\n",
    "    loss = 0.5 * tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric for constrastive loss because values close to 0 are equal and values high are different\n",
    "# 0.5 is the threshold here\n",
    "def constrastive_accuracy(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_true, tf.cast(y_pred < 0.5, y_true.dtype)), y_true.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    \"\"\"\n",
    "    Saves a model with a particular name\n",
    "    \"\"\"\n",
    "    model.save('models/' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese_network\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 44, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 44, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "siamese_model (Sequential)      (None, 256)          1534720     input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 256)          0           siamese_model[0][0]              \n",
      "                                                                 siamese_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            514         lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,535,234\n",
      "Trainable params: 1,535,234\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = siamese_network((MAX_LEN, EMBEDDING_SHAPE[0],))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=[X_train1, X_train2], y=Y_train, batch_size=128, epochs=80, validation_data=([X_val[0], X_val[1]], Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.3439 - accuracy: 0.8830\n",
      "test loss, test acc:  [0.343930117726326, 0.883]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "results = model.evaluate([X_test1, X_test2], Y_test, batch_size=16)\n",
    "print('test loss, test acc: ', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's name\n",
    "model_name = '0.1.2_Softmax-LSTM-128_batch_80_epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing\n",
    "Converts titles into embeddings arrays and allow the model to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_one = '2020 Dell XPS 13.3\" FHD Laptop Computer, Intel Core i5-10210U Processor, 8GB RAM, 512GB PCIe SSD, Baklit Keyboard, MaxxAudio, HD Webcam, Win 10, Platinum Silver, 32GB Snow Bell USB Card'\n",
    "title_two = '2020 Dell XPS 13.3\" Ultrabook, Intel Core i5-10230U, HD Webcam, (16GB RAM, 512GB PCIe SSD) Backlit, Windows 10, Silver'\n",
    "#title_one = '128GB ram'\n",
    "#title_two = '12gb ram'\n",
    "title_one_arr = np.zeros((1, MAX_LEN, 300))\n",
    "title_two_arr = np.zeros((1, MAX_LEN, 300))\n",
    "title_one = remove_stop_words(title_one.lower())\n",
    "title_two = remove_stop_words(title_two.lower())\n",
    "\n",
    "for idx, word in enumerate(title_one.split(' ')):\n",
    "    title_one_arr[0, idx] = fasttext_model[word]\n",
    "    \n",
    "for idx, word in enumerate(title_two.split(' ')):\n",
    "    title_two_arr[0, idx] = fasttext_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57954806, 0.420452  ]], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([title_one_arr, title_two_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
