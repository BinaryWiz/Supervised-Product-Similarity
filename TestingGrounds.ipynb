{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import random\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, Dropout, Lambda, Concatenate\n",
    "\n",
    "# Have to download the stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Get the fasttext model (we are using the largest one they offer [600B tokens])\n",
    "fasttext_model = fasttext.load_model('models/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Useful Function\n",
    "Functions that are continually used throughout this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LEN: 45 EMBEDDING_SHAPE: (300,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Definitions of some sizes in the training set\n",
    "\"\"\"\n",
    "MAX_LEN = 45\n",
    "EMBEDDING_SHAPE = (300,)\n",
    "print('MAX_LEN: ' + str(MAX_LEN), 'EMBEDDING_SHAPE: ' + str(EMBEDDING_SHAPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(df):\n",
    "    max_len = 0\n",
    "    for row in df.itertuples():\n",
    "        if len(row.title_one.split(' ')) > max_len:\n",
    "            max_len = len(row.title_one.split(' '))\n",
    "            \n",
    "        if len(row.title_two.split(' ')) > max_len:\n",
    "            max_len = len(row.title_two.split(' '))\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataframe(df):\n",
    "    for idx in range(len(df)):\n",
    "        print(df.iloc[idx].title_one + '\\n' + df.iloc[idx].title_two)\n",
    "        print('________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_data(pos_df, neg_df):\n",
    "    pos_df = pos_df.sample(frac=1)\n",
    "    neg_df = neg_df.sample(frac=1)\n",
    "    final_df = pd.concat([pos_df[:min(len(pos_df), len(neg_df))], neg_df[:min(len(pos_df), len(neg_df))]])\n",
    "    final_df = final_df.sample(frac=1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(phrase):\n",
    "    # Creates the stopwords\n",
    "    to_stop = stopwords.words('english')\n",
    "    to_stop.remove('m')\n",
    "    punctuation = \"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~ \\\"\"\n",
    "    for c in punctuation:\n",
    "        to_stop.append(c)\n",
    "\n",
    "    to_stop.append('null')\n",
    "    \n",
    "    for punc in punctuation:\n",
    "        phrase = phrase.replace(punc, ' ')\n",
    "    \n",
    "    return ' '.join((' '.join([x for x in phrase.split(' ') if x not in to_stop])).split()).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processsing and Organization\n",
    "Here, all we really want to do is prepare the data for training. This is **only** the data from **Gold Standard** This includes:\n",
    "* Simplifying the original data\n",
    "* Normalizing the data \n",
    "* Balancing the positive and negative examples\n",
    "* Creating the embedding representations that will actually get fed into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing and normalizing the data\n",
    "\"\"\"\n",
    "Essentially, we want to only have three attributes for each training example: title_one, title_two, label\n",
    "For normalization, we are just going to use the nltk stopwords and punctuation\n",
    "\"\"\"\n",
    "\n",
    "def preprocessing(orig_data):\n",
    "    \"\"\"\n",
    "    Normalizes the data by getting rid of stopwords and punctuation\n",
    "    \"\"\"\n",
    "    \n",
    "    # The new names of the columns\n",
    "    column_names = ['title_one', 'title_two', 'label']\n",
    "    # A new dataframe for the data we are going to be creating\n",
    "    norm_data = pd.DataFrame(columns = column_names)\n",
    "    # Iterate over the original dataframe (I know it is slow and there are probably better ways to do it)\n",
    "    iloc_data = orig_data.iloc\n",
    "    for idx in tqdm(range(len(orig_data))):\n",
    "        row = iloc_data[idx]\n",
    "        title_left = remove_stop_words(row.title_left)\n",
    "        title_right = remove_stop_words(row.title_right)\n",
    "        \n",
    "        # Append the newly created row (title_left, title_right, label) to the new dataframe\n",
    "        norm_data = norm_data.append(pd.DataFrame([[title_left, title_right, row.label]], columns=column_names))\n",
    "    \n",
    "    return norm_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_df(df):\n",
    "    \"\"\"\n",
    "    Returns a shuffled dataframe with an equal amount of positive and negative examples\n",
    "    \"\"\"\n",
    "    # Get the positive and negative examples\n",
    "    pos_df = df.loc[df['label'] == 1]\n",
    "    neg_df = df.loc[df['label'] == 0]\n",
    "    \n",
    "    # Shuffle the data\n",
    "    pos_df = pos_df.sample(frac=1)\n",
    "    neg_df = neg_df.sample(frac=1)\n",
    "    \n",
    "    # Concatenate the positive and negative examples and \n",
    "    # make sure there are only as many negative examples as positive examples\n",
    "    final_df = pd.concat([pos_df[:min(len(pos_df), len(neg_df))], neg_df[:min(len(pos_df), len(neg_df))]])\n",
    "    \n",
    "    # Shuffle the final data once again\n",
    "    final_df.sample(frac=1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(df, path):\n",
    "    \"\"\"\n",
    "    Creates and saves a simpler version of the original data that only contains the the two titles and the label.\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_bal_data = create_train_df(preprocessing(df))\n",
    "    \n",
    "    # Save the new normalized and simplified data to a CSV file to load later\n",
    "    norm_bal_data.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "computer_df = pd.read_json('data/train/computers_train_xlarge_normalized.json.gz', compression='gzip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>title_left</th>\n",
       "      <th>description_left</th>\n",
       "      <th>brand_left</th>\n",
       "      <th>price_left</th>\n",
       "      <th>specTableContent_left</th>\n",
       "      <th>keyValuePairs_left</th>\n",
       "      <th>category_left</th>\n",
       "      <th>cluster_id_left</th>\n",
       "      <th>identifiers_left</th>\n",
       "      <th>...</th>\n",
       "      <th>description_right</th>\n",
       "      <th>brand_right</th>\n",
       "      <th>price_right</th>\n",
       "      <th>specTableContent_right</th>\n",
       "      <th>keyValuePairs_right</th>\n",
       "      <th>category_right</th>\n",
       "      <th>cluster_id_right</th>\n",
       "      <th>identifiers_right</th>\n",
       "      <th>label</th>\n",
       "      <th>pair_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5490217</td>\n",
       "      <td>hp intel xeon x5560 prijzen tweakers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>categorie processors merk hp product hp intel ...</td>\n",
       "      <td>{'categorie': 'processors', 'merk': 'hp', 'pro...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>1679624</td>\n",
       "      <td>[{'/mpn': '[495906b21]'}, {'/gtin13': '[884420...</td>\n",
       "      <td>...</td>\n",
       "      <td>description intel xeon x5560 ml350 g6 2 80ghz ...</td>\n",
       "      <td>hp enterprise</td>\n",
       "      <td>usd 213 85</td>\n",
       "      <td>specifications category proliant processor sub...</td>\n",
       "      <td>{'category': 'proliant processor', 'sub catego...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>1679624</td>\n",
       "      <td>[{'/mpn': '[495906b21]'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>5490217#16248399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16876009</td>\n",
       "      <td>495906 b21 hp x5560 2 80ghz ml350 g6 , null ne...</td>\n",
       "      <td>description intel xeon x5560 ml350 g6 2 80ghz ...</td>\n",
       "      <td>hp enterprise</td>\n",
       "      <td>None</td>\n",
       "      <td>specifications category proliant processor sub...</td>\n",
       "      <td>{'category': 'proliant processor', 'sub catego...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>1679624</td>\n",
       "      <td>[{'/sku': '[495906b21]'}, {'/mpn': '[495906b21...</td>\n",
       "      <td>...</td>\n",
       "      <td>description intel xeon x5560 ml350 g6 2 80ghz ...</td>\n",
       "      <td>hp enterprise</td>\n",
       "      <td>usd 213 85</td>\n",
       "      <td>specifications category proliant processor sub...</td>\n",
       "      <td>{'category': 'proliant processor', 'sub catego...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>1679624</td>\n",
       "      <td>[{'/mpn': '[495906b21]'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>16876009#16248399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16543750</td>\n",
       "      <td>asus motherboard lga2066 ddr4 m 2 u atx 2xgbe ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>109916</td>\n",
       "      <td>[{'/sku': '[34181655]'}, {'/mpn': '[primex299d...</td>\n",
       "      <td>...</td>\n",
       "      <td>placa base atx socket lga2066 chipset intel x2...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>109916</td>\n",
       "      <td>[{'/productID': '[90mb0ty0m0eay0]'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>16543750#14031864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16721450</td>\n",
       "      <td>asus prime x299 deluxe prijzen tweakers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>categorie moederborden merk asus product asus ...</td>\n",
       "      <td>{'categorie': 'moederborden', 'merk': 'asus', ...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>109916</td>\n",
       "      <td>[{'/mpn': '[primex299deluxe,  90mb0ty0m0eay0]'...</td>\n",
       "      <td>...</td>\n",
       "      <td>support for x series intel core processors sli...</td>\n",
       "      <td>asus</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>109916</td>\n",
       "      <td>[{'/mpn': '[90mb0ty0m0eay0]'}, {'/gtin13': '[4...</td>\n",
       "      <td>1</td>\n",
       "      <td>16721450#10358026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14031864</td>\n",
       "      <td>asus prime x299 deluxe</td>\n",
       "      <td>placa base atx socket lga2066 chipset intel x2...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>109916</td>\n",
       "      <td>[{'/productID': '[90mb0ty0m0eay0]'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>atx quad channel ddr4 3 x pcie 3 0 x16 2 x m 2...</td>\n",
       "      <td>asus</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>109916</td>\n",
       "      <td>[{'/productID': '[asux29del]'}, {'/mpn': '[pri...</td>\n",
       "      <td>1</td>\n",
       "      <td>14031864#4588573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68456</th>\n",
       "      <td>11523636</td>\n",
       "      <td>seagate barracuda es 2 st3750330ns 750gb 3 5 h...</td>\n",
       "      <td>None</td>\n",
       "      <td>seagate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>987859</td>\n",
       "      <td>[{'/mpn': '[st3750330ns]'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>description 10 x 500gb hot plug serial ata sat...</td>\n",
       "      <td>hp enterprise</td>\n",
       "      <td>None</td>\n",
       "      <td>specifications category proliant harddrive sub...</td>\n",
       "      <td>{'category': 'proliant harddrive', 'sub catego...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>2516582</td>\n",
       "      <td>[{'/sku': '[45414100210pack]'}, {'/mpn': '[454...</td>\n",
       "      <td>0</td>\n",
       "      <td>11523636#13889537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68457</th>\n",
       "      <td>16852097</td>\n",
       "      <td>seagate barracuda es 2 750gb sata ii 32mb cach...</td>\n",
       "      <td>750gb capacity sata ii interface 7200rpm spin ...</td>\n",
       "      <td>seagate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>987859</td>\n",
       "      <td>[{'/mpn': '[st3750330ns]'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>description hp 6tb 3 5 inch lff serial attache...</td>\n",
       "      <td>hp enterprise</td>\n",
       "      <td>None</td>\n",
       "      <td>specifications category proliant harddrive sub...</td>\n",
       "      <td>{'category': 'proliant harddrive', 'sub catego...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>12211572</td>\n",
       "      <td>[{'/sku': '[782995001]'}, {'/mpn': '[782995001...</td>\n",
       "      <td>0</td>\n",
       "      <td>16852097#16101389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68458</th>\n",
       "      <td>11002668</td>\n",
       "      <td>null , st3750330ns seagate 750 gb 15k 3 5 3g sata</td>\n",
       "      <td>description 2 x seagate barracuda 750gb non ho...</td>\n",
       "      <td>seagate</td>\n",
       "      <td>cad 298 87 cad</td>\n",
       "      <td>specifications category seagate harddrive sub ...</td>\n",
       "      <td>{'category': 'seagate harddrive', 'sub categor...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>987859</td>\n",
       "      <td>[{'/mpn': '[st3750330ns]'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>the one drive for every desktop system need su...</td>\n",
       "      <td>seagate</td>\n",
       "      <td>us 46 99</td>\n",
       "      <td>official release date sep 30 2011 genre hard d...</td>\n",
       "      <td>{'official release date': 'sep 30 2011', 'genr...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>271241</td>\n",
       "      <td>[{'/mpn': '[st1000dm003]'}, {'/gtin13': '[7636...</td>\n",
       "      <td>0</td>\n",
       "      <td>11002668#14826173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68459</th>\n",
       "      <td>11002668</td>\n",
       "      <td>null , st3750330ns seagate 750 gb 15k 3 5 3g sata</td>\n",
       "      <td>description 2 x seagate barracuda 750gb non ho...</td>\n",
       "      <td>seagate</td>\n",
       "      <td>cad 298 87 cad</td>\n",
       "      <td>specifications category seagate harddrive sub ...</td>\n",
       "      <td>{'category': 'seagate harddrive', 'sub categor...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>987859</td>\n",
       "      <td>[{'/mpn': '[st3750330ns]'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>description 2 x 500gb hot plug serial ata sata...</td>\n",
       "      <td>hp enterprise</td>\n",
       "      <td>None</td>\n",
       "      <td>specifications category proliant harddrive sub...</td>\n",
       "      <td>{'category': 'proliant harddrive', 'sub catego...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>14324568</td>\n",
       "      <td>[{'/sku': '[4541410022pack]'}, {'/mpn': '[4541...</td>\n",
       "      <td>0</td>\n",
       "      <td>11002668#14249568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68460</th>\n",
       "      <td>10074696</td>\n",
       "      <td>seagate barracuda es 2 st3750330ns 750gb 3 5 h...</td>\n",
       "      <td>bid with confidence with our money back guaran...</td>\n",
       "      <td>seagate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>987859</td>\n",
       "      <td>[{'/mpn': '[st3750330ns]'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>description 10 x 500gb hot plug serial ata sat...</td>\n",
       "      <td>hp enterprise</td>\n",
       "      <td>None</td>\n",
       "      <td>specifications category proliant harddrive sub...</td>\n",
       "      <td>{'category': 'proliant harddrive', 'sub catego...</td>\n",
       "      <td>Computers_and_Accessories</td>\n",
       "      <td>2516582</td>\n",
       "      <td>[{'/sku': '[45414100210pack]'}, {'/mpn': '[454...</td>\n",
       "      <td>0</td>\n",
       "      <td>10074696#13889537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68461 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_left                                         title_left  \\\n",
       "0       5490217               hp intel xeon x5560 prijzen tweakers   \n",
       "1      16876009  495906 b21 hp x5560 2 80ghz ml350 g6 , null ne...   \n",
       "2      16543750  asus motherboard lga2066 ddr4 m 2 u atx 2xgbe ...   \n",
       "3      16721450            asus prime x299 deluxe prijzen tweakers   \n",
       "4      14031864                             asus prime x299 deluxe   \n",
       "...         ...                                                ...   \n",
       "68456  11523636  seagate barracuda es 2 st3750330ns 750gb 3 5 h...   \n",
       "68457  16852097  seagate barracuda es 2 750gb sata ii 32mb cach...   \n",
       "68458  11002668  null , st3750330ns seagate 750 gb 15k 3 5 3g sata   \n",
       "68459  11002668  null , st3750330ns seagate 750 gb 15k 3 5 3g sata   \n",
       "68460  10074696  seagate barracuda es 2 st3750330ns 750gb 3 5 h...   \n",
       "\n",
       "                                        description_left     brand_left  \\\n",
       "0                                                   None           None   \n",
       "1      description intel xeon x5560 ml350 g6 2 80ghz ...  hp enterprise   \n",
       "2                                                   None           None   \n",
       "3                                                   None           None   \n",
       "4      placa base atx socket lga2066 chipset intel x2...           None   \n",
       "...                                                  ...            ...   \n",
       "68456                                               None        seagate   \n",
       "68457  750gb capacity sata ii interface 7200rpm spin ...        seagate   \n",
       "68458  description 2 x seagate barracuda 750gb non ho...        seagate   \n",
       "68459  description 2 x seagate barracuda 750gb non ho...        seagate   \n",
       "68460  bid with confidence with our money back guaran...        seagate   \n",
       "\n",
       "           price_left                              specTableContent_left  \\\n",
       "0                None  categorie processors merk hp product hp intel ...   \n",
       "1                None  specifications category proliant processor sub...   \n",
       "2                None                                               None   \n",
       "3                None  categorie moederborden merk asus product asus ...   \n",
       "4                None                                               None   \n",
       "...               ...                                                ...   \n",
       "68456            None                                               None   \n",
       "68457            None                                               None   \n",
       "68458  cad 298 87 cad  specifications category seagate harddrive sub ...   \n",
       "68459  cad 298 87 cad  specifications category seagate harddrive sub ...   \n",
       "68460            None                                               None   \n",
       "\n",
       "                                      keyValuePairs_left  \\\n",
       "0      {'categorie': 'processors', 'merk': 'hp', 'pro...   \n",
       "1      {'category': 'proliant processor', 'sub catego...   \n",
       "2                                                   None   \n",
       "3      {'categorie': 'moederborden', 'merk': 'asus', ...   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "68456                                               None   \n",
       "68457                                               None   \n",
       "68458  {'category': 'seagate harddrive', 'sub categor...   \n",
       "68459  {'category': 'seagate harddrive', 'sub categor...   \n",
       "68460                                               None   \n",
       "\n",
       "                   category_left  cluster_id_left  \\\n",
       "0      Computers_and_Accessories          1679624   \n",
       "1      Computers_and_Accessories          1679624   \n",
       "2      Computers_and_Accessories           109916   \n",
       "3      Computers_and_Accessories           109916   \n",
       "4      Computers_and_Accessories           109916   \n",
       "...                          ...              ...   \n",
       "68456  Computers_and_Accessories           987859   \n",
       "68457  Computers_and_Accessories           987859   \n",
       "68458  Computers_and_Accessories           987859   \n",
       "68459  Computers_and_Accessories           987859   \n",
       "68460  Computers_and_Accessories           987859   \n",
       "\n",
       "                                        identifiers_left  ...  \\\n",
       "0      [{'/mpn': '[495906b21]'}, {'/gtin13': '[884420...  ...   \n",
       "1      [{'/sku': '[495906b21]'}, {'/mpn': '[495906b21...  ...   \n",
       "2      [{'/sku': '[34181655]'}, {'/mpn': '[primex299d...  ...   \n",
       "3      [{'/mpn': '[primex299deluxe,  90mb0ty0m0eay0]'...  ...   \n",
       "4                   [{'/productID': '[90mb0ty0m0eay0]'}]  ...   \n",
       "...                                                  ...  ...   \n",
       "68456                        [{'/mpn': '[st3750330ns]'}]  ...   \n",
       "68457                        [{'/mpn': '[st3750330ns]'}]  ...   \n",
       "68458                        [{'/mpn': '[st3750330ns]'}]  ...   \n",
       "68459                        [{'/mpn': '[st3750330ns]'}]  ...   \n",
       "68460                        [{'/mpn': '[st3750330ns]'}]  ...   \n",
       "\n",
       "                                       description_right    brand_right  \\\n",
       "0      description intel xeon x5560 ml350 g6 2 80ghz ...  hp enterprise   \n",
       "1      description intel xeon x5560 ml350 g6 2 80ghz ...  hp enterprise   \n",
       "2      placa base atx socket lga2066 chipset intel x2...           None   \n",
       "3      support for x series intel core processors sli...           asus   \n",
       "4      atx quad channel ddr4 3 x pcie 3 0 x16 2 x m 2...           asus   \n",
       "...                                                  ...            ...   \n",
       "68456  description 10 x 500gb hot plug serial ata sat...  hp enterprise   \n",
       "68457  description hp 6tb 3 5 inch lff serial attache...  hp enterprise   \n",
       "68458  the one drive for every desktop system need su...        seagate   \n",
       "68459  description 2 x 500gb hot plug serial ata sata...  hp enterprise   \n",
       "68460  description 10 x 500gb hot plug serial ata sat...  hp enterprise   \n",
       "\n",
       "      price_right                             specTableContent_right  \\\n",
       "0      usd 213 85  specifications category proliant processor sub...   \n",
       "1      usd 213 85  specifications category proliant processor sub...   \n",
       "2            None                                               None   \n",
       "3            None                                               None   \n",
       "4            None                                               None   \n",
       "...           ...                                                ...   \n",
       "68456        None  specifications category proliant harddrive sub...   \n",
       "68457        None  specifications category proliant harddrive sub...   \n",
       "68458    us 46 99  official release date sep 30 2011 genre hard d...   \n",
       "68459        None  specifications category proliant harddrive sub...   \n",
       "68460        None  specifications category proliant harddrive sub...   \n",
       "\n",
       "                                     keyValuePairs_right  \\\n",
       "0      {'category': 'proliant processor', 'sub catego...   \n",
       "1      {'category': 'proliant processor', 'sub catego...   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "68456  {'category': 'proliant harddrive', 'sub catego...   \n",
       "68457  {'category': 'proliant harddrive', 'sub catego...   \n",
       "68458  {'official release date': 'sep 30 2011', 'genr...   \n",
       "68459  {'category': 'proliant harddrive', 'sub catego...   \n",
       "68460  {'category': 'proliant harddrive', 'sub catego...   \n",
       "\n",
       "                  category_right cluster_id_right  \\\n",
       "0      Computers_and_Accessories          1679624   \n",
       "1      Computers_and_Accessories          1679624   \n",
       "2      Computers_and_Accessories           109916   \n",
       "3      Computers_and_Accessories           109916   \n",
       "4      Computers_and_Accessories           109916   \n",
       "...                          ...              ...   \n",
       "68456  Computers_and_Accessories          2516582   \n",
       "68457  Computers_and_Accessories         12211572   \n",
       "68458  Computers_and_Accessories           271241   \n",
       "68459  Computers_and_Accessories         14324568   \n",
       "68460  Computers_and_Accessories          2516582   \n",
       "\n",
       "                                       identifiers_right  label  \\\n",
       "0                              [{'/mpn': '[495906b21]'}]      1   \n",
       "1                              [{'/mpn': '[495906b21]'}]      1   \n",
       "2                   [{'/productID': '[90mb0ty0m0eay0]'}]      1   \n",
       "3      [{'/mpn': '[90mb0ty0m0eay0]'}, {'/gtin13': '[4...      1   \n",
       "4      [{'/productID': '[asux29del]'}, {'/mpn': '[pri...      1   \n",
       "...                                                  ...    ...   \n",
       "68456  [{'/sku': '[45414100210pack]'}, {'/mpn': '[454...      0   \n",
       "68457  [{'/sku': '[782995001]'}, {'/mpn': '[782995001...      0   \n",
       "68458  [{'/mpn': '[st1000dm003]'}, {'/gtin13': '[7636...      0   \n",
       "68459  [{'/sku': '[4541410022pack]'}, {'/mpn': '[4541...      0   \n",
       "68460  [{'/sku': '[45414100210pack]'}, {'/mpn': '[454...      0   \n",
       "\n",
       "                 pair_id  \n",
       "0       5490217#16248399  \n",
       "1      16876009#16248399  \n",
       "2      16543750#14031864  \n",
       "3      16721450#10358026  \n",
       "4       14031864#4588573  \n",
       "...                  ...  \n",
       "68456  11523636#13889537  \n",
       "68457  16852097#16101389  \n",
       "68458  11002668#14826173  \n",
       "68459  11002668#14249568  \n",
       "68460  10074696#13889537  \n",
       "\n",
       "[68461 rows x 22 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See some of the data. There is clearly a separation between the positive and negative examples\n",
    "computer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68461/68461 [07:42<00:00, 147.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create and save the data if the simple and normalized data does not exist\n",
    "computer_data_path = 'data/train/computers_train_bal_shuffle.csv'\n",
    "\n",
    "# If the computer data has not been made yet, make it\n",
    "if not os.path.exists(computer_data_path):\n",
    "    create_training_data(computer_df, computer_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cameras data\n",
    "camera_df = pd.read_json('data/train/cameras_train_xlarge_normalized.json.gz', compression='gzip', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the data if the simple and normalized data does not exist\n",
    "camera_data_path = 'data/train/cameras_train_bal_shuffle.csv'\n",
    "\n",
    "# If the computer data has not been made yet, make it\n",
    "if not os.path.exists(camera_data_path):\n",
    "    create_training_data(camera_df, camera_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_computer_df = pd.read_csv('data/train/computers_train_bal_shuffle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_one</th>\n",
       "      <th>title_two</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seagate barracuda 500gb internal hard drive ln...</td>\n",
       "      <td>seagate desktop hdd st500dm002 hard drive 500 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sandisk sdhc 16gb class 2 prijzen tweakers</td>\n",
       "      <td>sandisk sdsdb 016g crz 16gb sdhc card class 2 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asus radeon rx 460 dual oc 2048mb gddr5 pci ex...</td>\n",
       "      <td>grafica dual rx460 o2g tradineur com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>271837 004 hp 72 8 gb u320 scsi 10k wholesale ...</td>\n",
       "      <td>271837 004 hp 72 8 gb u320 scsi 10k</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359461 007 hp 300 gb 10k fc al hdd new wholesa...</td>\n",
       "      <td>359461 007 hp 300 gb 10k fc al new 2 pack whol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19375</th>\n",
       "      <td>376638 b21 hp 1gb pc3200 reg ecc kit</td>\n",
       "      <td>hp 397409 b21 2boy 512mbx2 240p pc2 5300 cl5 9...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19376</th>\n",
       "      <td>wd60efrx6tb disco duro 3 5 edici n red nas 64m...</td>\n",
       "      <td>wd purple surveillance 1tb 64 mb cache 3 5 sat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19377</th>\n",
       "      <td>bx80662i76700k intel 4 00ghz core i7 desktop p...</td>\n",
       "      <td>intel core i5 7600k 4x 3 80ghz boxed without c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19378</th>\n",
       "      <td>359623 b21 bl20p g2 xeon 2 8ghz 1m 512mb</td>\n",
       "      <td>345020 b21 bl20p g2 xeon 3 2ghz 1m 2p wholesal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19379</th>\n",
       "      <td>29347 b22 hp msl5026s2 super dlt</td>\n",
       "      <td>compaq dlt drive 2040 tape scsi series prices ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19380 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title_one  \\\n",
       "0      seagate barracuda 500gb internal hard drive ln...   \n",
       "1             sandisk sdhc 16gb class 2 prijzen tweakers   \n",
       "2      asus radeon rx 460 dual oc 2048mb gddr5 pci ex...   \n",
       "3      271837 004 hp 72 8 gb u320 scsi 10k wholesale ...   \n",
       "4      359461 007 hp 300 gb 10k fc al hdd new wholesa...   \n",
       "...                                                  ...   \n",
       "19375               376638 b21 hp 1gb pc3200 reg ecc kit   \n",
       "19376  wd60efrx6tb disco duro 3 5 edici n red nas 64m...   \n",
       "19377  bx80662i76700k intel 4 00ghz core i7 desktop p...   \n",
       "19378           359623 b21 bl20p g2 xeon 2 8ghz 1m 512mb   \n",
       "19379                   29347 b22 hp msl5026s2 super dlt   \n",
       "\n",
       "                                               title_two  label  \n",
       "0      seagate desktop hdd st500dm002 hard drive 500 ...      1  \n",
       "1      sandisk sdsdb 016g crz 16gb sdhc card class 2 ...      1  \n",
       "2                   grafica dual rx460 o2g tradineur com      1  \n",
       "3                    271837 004 hp 72 8 gb u320 scsi 10k      1  \n",
       "4      359461 007 hp 300 gb 10k fc al new 2 pack whol...      1  \n",
       "...                                                  ...    ...  \n",
       "19375  hp 397409 b21 2boy 512mbx2 240p pc2 5300 cl5 9...      0  \n",
       "19376  wd purple surveillance 1tb 64 mb cache 3 5 sat...      0  \n",
       "19377  intel core i5 7600k 4x 3 80ghz boxed without c...      0  \n",
       "19378  345020 b21 bl20p g2 xeon 3 2ghz 1m 2p wholesal...      0  \n",
       "19379  compaq dlt drive 2040 tape scsi series prices ...      0  \n",
       "\n",
       "[19380 rows x 3 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_computer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_camera_df = pd.read_csv('data/train/cameras_train_bal_shuffle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_camera_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laptop Data Preprocessing\n",
    "* Normalize the data\n",
    "* Create negative examples that represent when only a couple of attributes of the laptop data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the laptop data\n",
    "laptop_df = pd.read_csv('data/train/laptops.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price_euros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 2.3GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 640</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>1339.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Macbook Air</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1440x900</td>\n",
       "      <td>Intel Core i5 1.8GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics 6000</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>898.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HP</td>\n",
       "      <td>250 G6</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Full HD 1920x1080</td>\n",
       "      <td>Intel Core i5 7200U 2.5GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel HD Graphics 620</td>\n",
       "      <td>No OS</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>575.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>15.4</td>\n",
       "      <td>IPS Panel Retina Display 2880x1800</td>\n",
       "      <td>Intel Core i7 2.7GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>AMD Radeon Pro 455</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>2537.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 3.1GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 650</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>1803.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1316</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Yoga 500-14ISK</td>\n",
       "      <td>2 in 1 Convertible</td>\n",
       "      <td>14.0</td>\n",
       "      <td>IPS Panel Full HD / Touchscreen 1920x1080</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>4GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel HD Graphics 520</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.8kg</td>\n",
       "      <td>638.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1317</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Yoga 900-13ISK</td>\n",
       "      <td>2 in 1 Convertible</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Quad HD+ / Touchscreen 3200x1800</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>Intel HD Graphics 520</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.3kg</td>\n",
       "      <td>1499.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>1318</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>IdeaPad 100S-14IBR</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Celeron Dual Core N3050 1.6GHz</td>\n",
       "      <td>2GB</td>\n",
       "      <td>64GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.5kg</td>\n",
       "      <td>229.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>1319</td>\n",
       "      <td>HP</td>\n",
       "      <td>15-AC110nv (i7-6500U/6GB/1TB/Radeon</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>6GB</td>\n",
       "      <td>1TB HDD</td>\n",
       "      <td>AMD Radeon R5 M330</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>2.19kg</td>\n",
       "      <td>764.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>1320</td>\n",
       "      <td>Asus</td>\n",
       "      <td>X553SA-XX031T (N3050/4GB/500GB/W10)</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Celeron Dual Core N3050 1.6GHz</td>\n",
       "      <td>4GB</td>\n",
       "      <td>500GB HDD</td>\n",
       "      <td>Intel HD Graphics</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>2.2kg</td>\n",
       "      <td>369.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 Company                              Product  \\\n",
       "0              1   Apple                          MacBook Pro   \n",
       "1              2   Apple                          Macbook Air   \n",
       "2              3      HP                               250 G6   \n",
       "3              4   Apple                          MacBook Pro   \n",
       "4              5   Apple                          MacBook Pro   \n",
       "...          ...     ...                                  ...   \n",
       "1298        1316  Lenovo                       Yoga 500-14ISK   \n",
       "1299        1317  Lenovo                       Yoga 900-13ISK   \n",
       "1300        1318  Lenovo                   IdeaPad 100S-14IBR   \n",
       "1301        1319      HP  15-AC110nv (i7-6500U/6GB/1TB/Radeon   \n",
       "1302        1320    Asus  X553SA-XX031T (N3050/4GB/500GB/W10)   \n",
       "\n",
       "                TypeName  Inches                            ScreenResolution  \\\n",
       "0              Ultrabook    13.3          IPS Panel Retina Display 2560x1600   \n",
       "1              Ultrabook    13.3                                    1440x900   \n",
       "2               Notebook    15.6                           Full HD 1920x1080   \n",
       "3              Ultrabook    15.4          IPS Panel Retina Display 2880x1800   \n",
       "4              Ultrabook    13.3          IPS Panel Retina Display 2560x1600   \n",
       "...                  ...     ...                                         ...   \n",
       "1298  2 in 1 Convertible    14.0   IPS Panel Full HD / Touchscreen 1920x1080   \n",
       "1299  2 in 1 Convertible    13.3  IPS Panel Quad HD+ / Touchscreen 3200x1800   \n",
       "1300            Notebook    14.0                                    1366x768   \n",
       "1301            Notebook    15.6                                    1366x768   \n",
       "1302            Notebook    15.6                                    1366x768   \n",
       "\n",
       "                                       Cpu   Ram               Memory  \\\n",
       "0                     Intel Core i5 2.3GHz   8GB            128GB SSD   \n",
       "1                     Intel Core i5 1.8GHz   8GB  128GB Flash Storage   \n",
       "2               Intel Core i5 7200U 2.5GHz   8GB            256GB SSD   \n",
       "3                     Intel Core i7 2.7GHz  16GB            512GB SSD   \n",
       "4                     Intel Core i5 3.1GHz   8GB            256GB SSD   \n",
       "...                                    ...   ...                  ...   \n",
       "1298            Intel Core i7 6500U 2.5GHz   4GB            128GB SSD   \n",
       "1299            Intel Core i7 6500U 2.5GHz  16GB            512GB SSD   \n",
       "1300  Intel Celeron Dual Core N3050 1.6GHz   2GB   64GB Flash Storage   \n",
       "1301            Intel Core i7 6500U 2.5GHz   6GB              1TB HDD   \n",
       "1302  Intel Celeron Dual Core N3050 1.6GHz   4GB            500GB HDD   \n",
       "\n",
       "                               Gpu       OpSys  Weight  Price_euros  \n",
       "0     Intel Iris Plus Graphics 640       macOS  1.37kg      1339.69  \n",
       "1           Intel HD Graphics 6000       macOS  1.34kg       898.94  \n",
       "2            Intel HD Graphics 620       No OS  1.86kg       575.00  \n",
       "3               AMD Radeon Pro 455       macOS  1.83kg      2537.45  \n",
       "4     Intel Iris Plus Graphics 650       macOS  1.37kg      1803.60  \n",
       "...                            ...         ...     ...          ...  \n",
       "1298         Intel HD Graphics 520  Windows 10   1.8kg       638.00  \n",
       "1299         Intel HD Graphics 520  Windows 10   1.3kg      1499.00  \n",
       "1300             Intel HD Graphics  Windows 10   1.5kg       229.00  \n",
       "1301            AMD Radeon R5 M330  Windows 10  2.19kg       764.00  \n",
       "1302             Intel HD Graphics  Windows 10   2.2kg       369.00  \n",
       "\n",
       "[1303 rows x 13 columns]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will be used in order to exchange the different attributes\n",
    "# to create negative examples\n",
    "class LaptopAttributes():\n",
    "    company = {'Apple'}\n",
    "    product = {'MacBook Pro'}\n",
    "    inches = {'13.3'}\n",
    "    cpu = {'Intel Core i5 2.3GHz'}\n",
    "    ram = {'4GB'}\n",
    "    memory = {'256GB SSD'}\n",
    "    gpu = {'Intel HD Graphics 520'}\n",
    "    screen = {'1440x900'}\n",
    "    \n",
    "    def get_all_data():\n",
    "        return {\n",
    "            'company': LaptopAttributes.company,\n",
    "            'product': LaptopAttributes.product,\n",
    "            'inches': LaptopAttributes.inches,\n",
    "            'cpu': LaptopAttributes.cpu,\n",
    "            'ram': LaptopAttributes.ram,\n",
    "            'memory': LaptopAttributes.memory,\n",
    "            'gpu': LaptopAttributes.gpu,\n",
    "            'screen': LaptopAttributes.screen\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attribute sets\n",
    "def create_attribute_sets(df):\n",
    "    LaptopAttributes.company.update([row.Company for row in laptop_df[['Company']].itertuples()])\n",
    "    LaptopAttributes.product.update([row.Product for row in laptop_df[['Product']].itertuples()])\n",
    "    LaptopAttributes.inches.update([str(row.Inches) for row in laptop_df[['Inches']].itertuples()])\n",
    "    LaptopAttributes.cpu.update([row.Cpu for row in laptop_df[['Cpu']].itertuples()])\n",
    "    LaptopAttributes.ram.update([row.Ram for row in laptop_df[['Ram']].itertuples()])\n",
    "    LaptopAttributes.memory.update([row.Memory for row in laptop_df[['Memory']].itertuples()])\n",
    "    LaptopAttributes.gpu.update([row.Gpu for row in laptop_df[['Gpu']].itertuples()])\n",
    "    LaptopAttributes.screen.update([row.ScreenResolution for row in laptop_df[['ScreenResolution']].itertuples()])\n",
    "\n",
    "create_attribute_sets(laptop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_row(row):\n",
    "    # Note: got rid of everything after the '(' because it has info about the actual specs of the laptop\n",
    "    # so if we change the specs, we need to fix that too\n",
    "    \n",
    "    # Special tags at the end of the amount of inches of the laptop and the RAM to simulate real data\n",
    "    inch_attr = str(row['Inches']) + random.choice([' inch', '', '\"'])\n",
    "    ram_attr = row['Ram'] + random.choice([' ram', ' memory', ''])\n",
    "    \n",
    "    # These are words that commonly come up with laptops\n",
    "    modifiers = ['premium', 'new', 'fast', 'latest model']\n",
    "    add_ins = ['USB 3.0', 'USB 3.1 Type-C', 'USB Type-C', 'Bluetooth', 'WIFI', 'Webcam', 'FP Reader',\n",
    "               'HDMI', '802.11ac', '802.11 ac', 'home', 'flagship', 'business', 'GbE LAN', 'DVD-RW', 'DVD', 'Windows 10']\n",
    "    \n",
    "    cpu_attr = row['Cpu']\n",
    "    if random.choice([0, 1]):\n",
    "        cpu_attr = cpu_attr.split(' ')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'Intel' in cpu_attr:\n",
    "                cpu_attr.remove('Intel')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'Core' in cpu_attr:\n",
    "                cpu_attr.remove('Core')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'AMD' in cpu_attr:\n",
    "                cpu_attr.remove('AMD')\n",
    "    \n",
    "        cpu_attr = ' '.join(cpu_attr)\n",
    "\n",
    "    # Create a list for all the product attributes\n",
    "    order_attrs = [random.choice(modifiers),\n",
    "                   row['Company'],\n",
    "                   row['Product'].split('(')[0],\n",
    "                   row['TypeName'],\n",
    "                   inch_attr,\n",
    "                   row['ScreenResolution'],\n",
    "                   cpu_attr,\n",
    "                   ram_attr,\n",
    "                   row['Memory'],\n",
    "                   row['Gpu']]\n",
    "    \n",
    "    order_attrs = order_attrs + random.sample(add_ins, random.choice([1, 2, 3, 4]))\n",
    "    \n",
    "    # Shuffle the data because in real data, it does not really matter what order the attributes are in\n",
    "    random.shuffle(order_attrs)\n",
    "    \n",
    "    return ' '.join(order_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the negative examples for the laptop data\n",
    "# The laptop_df is the original data, the new_df is the dataframe to append the new data to\n",
    "# and the attributes are the attributes to swap for the new data\n",
    "def create_neg_laptop_data(laptop_df, attributes):\n",
    "    new_column_names = ['title_one', 'title_two', 'label']\n",
    "    negative_df = pd.DataFrame(columns = new_column_names)\n",
    "    for row in tqdm(range(len(laptop_df))):\n",
    "        # Create a copy of the row for the negative example\n",
    "        neg_row = laptop_df.iloc[row]\n",
    "        for attribute_class in attributes:\n",
    "            # Get the row in the laptop_data\n",
    "            orig_row = laptop_df.iloc[row]\n",
    "            \n",
    "            # Get the attribute that we are trying to change\n",
    "            attribute_val = orig_row[attribute_class]\n",
    "            \n",
    "            # Temporarily value for the new value\n",
    "            new_val = attribute_val\n",
    "            \n",
    "            # Make sure we really get a new attribute\n",
    "            while new_val == attribute_val:\n",
    "                new_val = random.sample(LaptopAttributes.get_all_data()[attribute_class.lower()], 1)[0]\n",
    "            \n",
    "            # Change the value in the neg_row to the new value\n",
    "            neg_row[attribute_class] = new_val\n",
    "            \n",
    "            # Concatenate and normalize the data\n",
    "            title_one = remove_stop_words(concatenate_row(orig_row).lower())\n",
    "            title_two = remove_stop_words(concatenate_row(neg_row).lower())\n",
    "            \n",
    "            # Append the data to the new df\n",
    "            negative_df = negative_df.append(pd.DataFrame([[title_one, title_two, 0]], columns=new_column_names))\n",
    "    \n",
    "    return negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1303 [00:00<?, ?it/s]/home/jason/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 1303/1303 [00:14<00:00, 88.83it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_df = create_neg_laptop_data(laptop_df, attributes=['Cpu', 'Memory', 'Ram', 'Inches', 'Product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe(neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the postive examples for the laptop data\n",
    "# The laptop_df is the original data, the new_df is the dataframe to append the new data to\n",
    "# and the attributes are the attributes to swap or delete for the new data\n",
    "def create_pos_laptop_data(laptop_df, rm_attrs, add_attrs):\n",
    "    new_column_names = ['title_one', 'title_two', 'label']\n",
    "    pos_df = pd.DataFrame(columns = new_column_names)\n",
    "    for row in tqdm(range(len(laptop_df))):\n",
    "        # Remove the attribute from the new title\n",
    "        for attr_list in rm_attrs:\n",
    "            # Create a copy of the row for the negative example\n",
    "            new_row = laptop_df.iloc[row]\n",
    "            orig_row = laptop_df.iloc[row]\n",
    "            for attr in attr_list:\n",
    "                new_row[attr] = ''\n",
    "        \n",
    "            title_one = remove_stop_words(concatenate_row(orig_row).lower())\n",
    "            title_two = remove_stop_words(concatenate_row(new_row).lower())\n",
    "\n",
    "            pos_df = pos_df.append(pd.DataFrame([[title_one, title_two, 1]], columns=new_column_names))\n",
    "\n",
    "    return pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1303 [00:00<?, ?it/s]/home/jason/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "100%|██████████| 1303/1303 [00:15<00:00, 85.07it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_df = create_pos_laptop_data(laptop_df, rm_attrs = [['Company'], ['TypeName'], ['ScreenResolution'], ['Product'], ['TypeName', 'ScreenResolution']], add_attrs = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_laptop_df = create_final_data(pos_df, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_laptop_df = final_laptop_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCPartPicker Data\n",
    "* Organize the data\n",
    "* Preprocess the data\n",
    "* Create negative and positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_df = pd.read_csv('data/train/pos_ram_titles.csv')\n",
    "cpu_df = pd.read_csv('data/train/pos_cpu_titles.csv')\n",
    "hard_drive_df = pd.read_csv('data/train/pos_hard_drive_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_drive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Unnamed: 0 column and drop any row where it is all NaN\n",
    "def remove_misc(df):\n",
    "    columns = list(df.columns)[1:]\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "    df = df.dropna(how='all')\n",
    "    print(len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "315\n",
      "233\n"
     ]
    }
   ],
   "source": [
    "ram_df = remove_misc(ram_df)\n",
    "cpu_df = remove_misc(cpu_df)\n",
    "hard_drive_df = remove_misc(hard_drive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_pcpartpicker_data(df):\n",
    "    columns = list(df.columns)\n",
    "    pos_df = pd.DataFrame(columns=['title_one', 'title_two', 'label'])\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        row = df.iloc()[idx]\n",
    "        titles = []\n",
    "        for col in columns:\n",
    "            if not pd.isnull(row[col]): titles.append(remove_stop_words(row[col]))\n",
    "        if len(titles) > 1:\n",
    "            combs = combinations(titles, 2)\n",
    "            for comb in combs:\n",
    "                comb = list(comb)\n",
    "                comb.append(1)\n",
    "                pos_df = pos_df.append(pd.DataFrame([comb], columns=['title_one', 'title_two', 'label']))\n",
    "    \n",
    "    return pos_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:00<00:00, 365.06it/s]\n",
      "100%|██████████| 315/315 [00:00<00:00, 391.02it/s]\n",
      "100%|██████████| 233/233 [00:00<00:00, 285.44it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_ram_data = generate_pos_pcpartpicker_data(ram_df)\n",
    "\n",
    "pos_cpu_data = generate_pos_pcpartpicker_data(cpu_df)\n",
    "\n",
    "pos_hard_drive_data = generate_pos_pcpartpicker_data(hard_drive_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_pcpartpicker_data(df):\n",
    "    columns = list(df.columns)\n",
    "    neg_df = pd.DataFrame(columns=['title_one', 'title_two', 'label'])\n",
    "    df_list = df.iloc()\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        row = df_list[idx]\n",
    "        for col in columns:\n",
    "            if not pd.isnull(row[col]):\n",
    "                neg_idx = None\n",
    "                while neg_idx == idx or neg_idx is None:\n",
    "                    neg_idx = random.randint(0, len(df) - 1)\n",
    "                \n",
    "                neg_title = None\n",
    "                while neg_title == None or pd.isnull(neg_title):\n",
    "                    neg_title = df_list[neg_idx][random.choice(columns)]\n",
    "                \n",
    "                neg_title = remove_stop_words(neg_title)\n",
    "                \n",
    "                neg_df = neg_df.append(pd.DataFrame([[remove_stop_words(row[col]), neg_title, 0]], columns=['title_one', 'title_two', 'label']))\n",
    "    \n",
    "    return neg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:00<00:00, 221.99it/s]\n",
      "100%|██████████| 315/315 [00:01<00:00, 264.64it/s]\n",
      "100%|██████████| 233/233 [00:01<00:00, 218.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962 696 1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "neg_ram_data = generate_neg_pcpartpicker_data(ram_df)\n",
    "\n",
    "neg_cpu_data = generate_neg_pcpartpicker_data(cpu_df)\n",
    "\n",
    "neg_hard_drive_data = generate_neg_pcpartpicker_data(hard_drive_df)\n",
    "\n",
    "final_ram_data = create_final_data(pos_ram_data, neg_ram_data)\n",
    "\n",
    "final_cpu_data = create_final_data(pos_cpu_data, neg_cpu_data)\n",
    "\n",
    "final_hard_drive_data = create_final_data(pos_hard_drive_data, neg_hard_drive_data)\n",
    "\n",
    "print(len(final_cpu_data), len(final_ram_data), len(final_hard_drive_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Computer Spec Generation\n",
    "* Using the PCPartPicker data, we combine computer parts (e.g. CPU, hard drive, RAM, etc.) and create a dataframe from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "video_card_df = pd.read_csv('data/train/video-cards-data.csv')\n",
    "ram_df = pd.read_csv('data/train/ram_data.csv')\n",
    "cpu_df = pd.read_csv('data/train/cpu_data.csv')\n",
    "hard_drive_df = pd.read_csv('data/train/hard_drive_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecAttributes():\n",
    "    video_card = {'GeForce RTX 2070'}\n",
    "    ram = [str(x) + ' GB' for x in range(2, 130, 2)]\n",
    "    hard_drive = [str(x) + ' GB' for x in range(120, 513, 8)] + [str(x) + ' TB' for x in range(1, 8)]\n",
    "    cpu = {}\n",
    "    laptop_brands = ['Lenovo ThinkPad', 'Lenovo ThinkBook', 'Lenovo IdeaPad', 'Lenovo Yoga', 'Lenovo Legion', 'HP Envy', 'HP Chromebook', 'HP Spectre', 'HP ZBook', 'HP Probook', 'HP Elitebook', 'HP Pavilion', 'HP Omen', 'Dell Alienware', 'Dell Vostro', 'Dell Inspiron', 'Dell Latitude', 'Dell XPS', 'Dell G Series', 'Dell Precision', 'Apple Macbook', 'Apple Macbook Air', 'Apple Mac', 'Acer Aspire', 'Acer swift', 'Acer Spin', 'Acer Switch', 'Acer Extensa', 'Acer Travelmate', 'Acer Nitro', 'Acer Enduro', 'Acer Predator', 'Asus ZenBook', 'Asus Vivobook', 'Asus Republic of Gamers', 'Asus ROG', 'Asus TUF GAMING']\n",
    "    \n",
    "    def get_all_data():\n",
    "        return {\n",
    "            'cpu': SpecAttributes.cpu.keys(),\n",
    "            'ram': SpecAttributes.ram,\n",
    "            'hard_drive': SpecAttributes.hard_drive,\n",
    "            'video_card': SpecAttributes.video_card,\n",
    "            'laptop_brands': SpecAttributes.laptop_brands\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the CPU data into SpecAttrbutes\n",
    "temp_iloc = cpu_df.iloc()\n",
    "for idx in range(len(cpu_df)):\n",
    "    row = temp_iloc[idx]\n",
    "    SpecAttributes.cpu[row['name']] = [row['cores'], row['core_clock']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the video card data into SpecAttributes\n",
    "temp_iloc = video_card_df.iloc()\n",
    "for idx in range(len(video_card_df)):\n",
    "    row = temp_iloc[idx]\n",
    "    SpecAttributes.video_card.update([row['chipset']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = np.meshgrid(*[SpecAttributes.laptop_brands, list(SpecAttributes.cpu.keys()), SpecAttributes.hard_drive, SpecAttributes.ram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = np.array(combos).T.reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13092672"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13092672, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=combos, columns=['brand', 'cpu', 'hard_drive', 'ram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13092672"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/train/spec_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_df = pd.read_csv('data/train/spec_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Computer Training Creation\n",
    "* Create the positive and negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = ['title_one', 'title_two', 'label']\n",
    "hard_drive_types = ['HDD', 'Hard Drive', 'Internal Hard Drive']\n",
    "ssd_types = ['SSD', 'Solid State Drive', 'M.2 SSD', 'SATA SSD']\n",
    "neg_spec_df = pd.DataFrame(columns = new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_spec_data(row):\n",
    "    # Special tags at the end of the amount of inches of the laptop and the RAM to simulate real data\n",
    "    inch_attr = str(row['inches']) + random.choice([' inch', '', '\"'])\n",
    "    ram_attr = row['ram'] + random.choice([' ram', ' memory', ''])\n",
    "    \n",
    "    # These are words that commonly come up with laptops\n",
    "    modifiers = ['premium', 'new', 'fast', 'latest model']\n",
    "    add_ins = ['USB 3.0', 'USB 3.1 Type-C', 'USB Type-C', 'Bluetooth', 'WIFI', 'Webcam', 'FP Reader',\n",
    "               'HDMI', '802.11ac', '802.11 ac', 'home', 'flagship', 'business', 'GbE LAN', 'DVD-RW',\n",
    "               'DVD', 'Windows 10']\n",
    "\n",
    "    cpu_attr = row['cpu']\n",
    "    cores = SpecAttributes.cpu[cpu_attr][0]\n",
    "    ghz = SpecAttributes.cpu[cpu_attr][1]\n",
    "    \n",
    "    if random.random() > 0.5:\n",
    "        cpu_attr = cpu_attr.split(' ')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'Intel' in cpu_attr:\n",
    "                cpu_attr.remove('Intel')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'Core' in cpu_attr:\n",
    "                cpu_attr.remove('Core')\n",
    "        if random.choice([0, 1]):\n",
    "            if 'AMD' in cpu_attr:\n",
    "                cpu_attr.remove('AMD')\n",
    "    \n",
    "        cpu_attr = ' '.join(cpu_attr)\n",
    "    \n",
    "    # Random chance of putting the cores in the CPU attribute\n",
    "    if random.random() > 0.7:\n",
    "        cpu_attr = '{} {} {}'.format(cpu_attr, cores, 'Core')\n",
    "    \n",
    "    # Random chance of putting the GHz in the CPU attribute\n",
    "    if random.random() > 0.7:\n",
    "        cpu_attr = '{} {}'.format(cpu_attr, ghz)\n",
    "    \n",
    "    if random.random() > 0.55:\n",
    "        cpu_attr = '{} {}'.format(cpu_attr, 'CPU')\n",
    "    \n",
    "    # Create a list for all the product attributes\n",
    "    order_attrs = [random.choice(modifiers),\n",
    "                   row['company'],\n",
    "                   row['product'],\n",
    "                   row['hard_drive'],\n",
    "                   row['screen'],\n",
    "                   inch_attr,\n",
    "                   cpu_attr,\n",
    "                   ram_attr\n",
    "                  ]\n",
    "    \n",
    "    order_attrs = order_attrs + random.sample(add_ins, random.choice([1, 2, 3, 4]))\n",
    "    \n",
    "    # Shuffle the data because in real data, it does not really matter what order the attributes are in\n",
    "    random.shuffle(order_attrs)\n",
    "    \n",
    "    return ' '.join(order_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the negative examples for the laptop data\n",
    "# The laptop_df is the original data, the new_df is the dataframe to append the new data to\n",
    "# and the attributes are the attributes to swap for the new data\n",
    "def create_neg_spec_laptop(df, attributes):\n",
    "    global neg_spec_df\n",
    "    \n",
    "    df_iloc = df.iloc()\n",
    "    for row in tqdm(range(len(df))):\n",
    "        # Create a copy of the row for the negative example\n",
    "        for attribute_class in attributes:\n",
    "            neg_row = df_iloc[row]\n",
    "            # Get the row in the laptop_data and add the inch attribute\n",
    "            orig_row = df_iloc[row]\n",
    "            \n",
    "            # Set product and company\n",
    "            orig_row['company'] = orig_row['brand'].split(' ', 1)[0]\n",
    "            orig_row['product'] = orig_row['brand'].split(' ', 1)[1]\n",
    "            neg_row['company'] = orig_row['brand'].split(' ', 1)[0]\n",
    "            neg_row['product'] = orig_row['brand'].split(' ', 1)[1]\n",
    "            \n",
    "            # Get a random inch attribute\n",
    "            inch_attr = random.choice(list(LaptopAttributes.inches))\n",
    "            \n",
    "            # Get random screen attribute\n",
    "            screen_attr = random.choice(list(LaptopAttributes.screen))\n",
    "            \n",
    "            # Set the attributes\n",
    "            orig_row['inches'] = inch_attr\n",
    "            neg_row['inches'] = inch_attr\n",
    "            orig_row['screen'] = screen_attr\n",
    "            neg_row['screen'] = screen_attr\n",
    "            \n",
    "            if attribute_class == 'inches':\n",
    "                # New inch attribute\n",
    "                new_inch_attr = inch_attr\n",
    "\n",
    "                # If the original attribute is still the same, keep getting a random one\n",
    "                while inch_attr == new_inch_attr:\n",
    "                    new_inch_attr = random.choice(list(LaptopAttributes.inches))\n",
    "                \n",
    "                neg_row['inches'] = new_inch_attr\n",
    "            \n",
    "            elif attribute_class == 'screen':\n",
    "                # Have screen attr\n",
    "                orig_screen_attr = random.choice(list(LaptopAttributes.screen))\n",
    "                \n",
    "                # New screen attribute\n",
    "                new_screen_attr = screen_attr\n",
    "                \n",
    "                # If the original attribute is still the same, keep getting a random one\n",
    "                while orig_screen_attr == new_screen_attr:\n",
    "                    new_screen_attr = random.choice(list(LaptopAttributes.screen))\n",
    "                \n",
    "                neg_row['screen'] = new_screen_attr\n",
    "                orig_row['screen'] = orig_screen_attr\n",
    "            \n",
    "            elif attribute_class == 'product':\n",
    "                # New product attr\n",
    "                new_product_attr = orig_row['product']\n",
    "                \n",
    "                # If the original attribute is still the same, keep getting a random one\n",
    "                while orig_row['product'] == new_product_attr:\n",
    "                    new_product_attr = random.choice(SpecAttributes.laptop_brands).split(' ', 1)[1]\n",
    "                \n",
    "                neg_row['product'] = new_product_attr\n",
    "            \n",
    "            elif attribute_class == 'hard_drive':\n",
    "                # New drive attribute\n",
    "                new_drive_attr = orig_row['hard_drive']\n",
    "                \n",
    "                # If the original attribute is still the same, keep getting a random one\n",
    "                while orig_row['hard_drive'] == new_drive_attr:\n",
    "                    new_drive_attr = random.choice(SpecAttributes.hard_drive)\n",
    "                \n",
    "                neg_row['hard_drive'] = '{} {}'.format(new_drive_attr, random.choice([random.choice(hard_drive_types), random.choice(ssd_types)]))\n",
    "                orig_row['hard_drive'] = '{} {}'.format(orig_row['hard_drive'], random.choice([random.choice(hard_drive_types), random.choice(ssd_types)]))\n",
    "            \n",
    "            else:\n",
    "                # Get the attribute that we are trying to change\n",
    "                attribute_val = orig_row[attribute_class]\n",
    "\n",
    "                # Temporarily value for the new value\n",
    "                new_val = attribute_val\n",
    "\n",
    "                # Make sure we really get a new attribute\n",
    "                while new_val == attribute_val:\n",
    "                    new_val = random.sample(SpecAttributes.get_all_data()[attribute_class.lower()], 1)[0]\n",
    "\n",
    "                # Change the value in the neg_row to the new value\n",
    "                neg_row[attribute_class] = new_val\n",
    "            \n",
    "            # We still need to add the phrasing to the hard drive attribute if it is not the current attribute class\n",
    "            if attribute_class != 'hard_drive':\n",
    "                drive_type = random.choice([random.choice(hard_drive_types), random.choice(ssd_types)])\n",
    "                neg_row['hard_drive'] = '{} {}'.format(neg_row['hard_drive'], drive_type)\n",
    "                orig_row['hard_drive'] = '{} {}'.format(orig_row['hard_drive'], drive_type)\n",
    "            \n",
    "            # Concatenate and normalize the data\n",
    "            title_one = remove_stop_words(concatenate_spec_data(orig_row).lower())\n",
    "            title_two = remove_stop_words(concatenate_spec_data(neg_row).lower())\n",
    "            \n",
    "            # Append the data to the new df\n",
    "            neg_spec_df = neg_spec_df.append(pd.DataFrame([[title_one, title_two, 0]], columns=new_column_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_neg_spec_laptop(spec_df, ['cpu', 'ram', 'hard_drive', 'product', 'inches', 'screen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33080"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe(neg_spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_spec_df = pd.DataFrame(columns = new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the postive examples for the laptop data\n",
    "# The laptop_df is the original data, the new_df is the dataframe to append the new data to\n",
    "# and the attributes are the attributes to swap or delete for the new data\n",
    "def create_pos_spec_data(df, rm_attrs, add_attrs):\n",
    "    global pos_spec_df\n",
    "    df_iloc = df.iloc()\n",
    "    new_column_names = ['title_one', 'title_two', 'label']\n",
    "    for row in tqdm(range(len(df))):\n",
    "        # Set the new row to the same as the original to begin changing it\n",
    "        new_row = df_iloc[row]\n",
    "\n",
    "        # Get the row in the df and add the inch attribute\n",
    "        orig_row = df_iloc[row]\n",
    "\n",
    "        # Set product and company\n",
    "        orig_row['company'] = orig_row['brand'].split(' ', 1)[0]\n",
    "        orig_row['product'] = orig_row['brand'].split(' ', 1)[1]\n",
    "        new_row['company'] = orig_row['brand'].split(' ', 1)[0]\n",
    "        new_row['product'] = orig_row['brand'].split(' ', 1)[1]\n",
    "\n",
    "        # Get a random inch attribute\n",
    "        inch_attr = random.choice(list(LaptopAttributes.inches))\n",
    "\n",
    "        # Get random screen attribute\n",
    "        screen_attr = random.choice(list(LaptopAttributes.screen))\n",
    "\n",
    "        # Get random hard drive attribute and type\n",
    "        hard_drive_attr = random.choice(list(SpecAttributes.hard_drive))\n",
    "        \n",
    "        # Get whether it will be an ssd or a hard drive\n",
    "        drive_type = random.choice([hard_drive_types, ssd_types])\n",
    "\n",
    "        # Set the attributes\n",
    "        orig_row['inches'] = inch_attr\n",
    "        orig_row['screen'] = screen_attr\n",
    "        orig_row['hard_drive'] = '{} {}'.format(hard_drive_attr, random.choice(drive_type))\n",
    "        new_row['inches'] = inch_attr\n",
    "        new_row['screen'] = screen_attr\n",
    "        new_row['hard_drive'] = '{} {}'.format(hard_drive_attr, random.choice(drive_type))\n",
    "        \n",
    "        for attr_list in rm_attrs:\n",
    "            # Simply create a copy of new_row so that we do not have to keep on generating the same thing\n",
    "            pos_row = new_row.copy()\n",
    "            \n",
    "            for attr in attr_list:\n",
    "                pos_row[attr] = ''\n",
    "        \n",
    "            title_one = remove_stop_words(concatenate_spec_data(orig_row).lower())\n",
    "            title_two = remove_stop_words(concatenate_spec_data(pos_row).lower())\n",
    "            pos_spec_df = pos_spec_df.append(pd.DataFrame([[title_one, title_two, 1]], columns=new_column_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = create_pos_spec_data(spec_df, rm_attrs = [['company'], ['product'], ['screen'], ['product', 'screen'], ['company', 'screen']], add_attrs = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35116"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe(pos_spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spec_df = create_final_data(pos_spec_df, neg_spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66160"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spec_df.to_csv('data/train/spec_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spec_df = pd.read_csv('data/train/spec_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Data Creation\n",
    "* Create a differentiation between different gigabytes of storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = ['title_one', 'title_two', 'label']\n",
    "pos_gb_df = pd.DataFrame(columns = new_column_names)\n",
    "neg_gb_df = pd.DataFrame(columns = new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for x in range(2, 5000, 2):\n",
    "    attr = '{} {}'.format(x, 'gb')\n",
    "    pos.append([attr, attr, 1])\n",
    "\n",
    "pos_gb_df = pos_gb_df.append(pd.DataFrame(pos, columns = new_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "for x in range(2, 1000, 2):\n",
    "    for y in range(2, 1000, 2):\n",
    "        x_attr = '{} {}'.format(x, 'gb')\n",
    "        y_attr = '{} {}'.format(y, 'gb')\n",
    "\n",
    "        if x != y:\n",
    "            neg.append([x_attr, y_attr, 0])\n",
    "\n",
    "neg_gb_df = neg_gb_df.append(pd.DataFrame(neg, columns = new_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_one</th>\n",
       "      <th>title_two</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 gb</td>\n",
       "      <td>2 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 gb</td>\n",
       "      <td>4 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6 gb</td>\n",
       "      <td>6 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8 gb</td>\n",
       "      <td>8 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 gb</td>\n",
       "      <td>10 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>4990 gb</td>\n",
       "      <td>4990 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>4992 gb</td>\n",
       "      <td>4992 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>4994 gb</td>\n",
       "      <td>4994 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>4996 gb</td>\n",
       "      <td>4996 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>4998 gb</td>\n",
       "      <td>4998 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2499 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     title_one title_two label\n",
       "0         2 gb      2 gb     1\n",
       "1         4 gb      4 gb     1\n",
       "2         6 gb      6 gb     1\n",
       "3         8 gb      8 gb     1\n",
       "4        10 gb     10 gb     1\n",
       "...        ...       ...   ...\n",
       "2494   4990 gb   4990 gb     1\n",
       "2495   4992 gb   4992 gb     1\n",
       "2496   4994 gb   4994 gb     1\n",
       "2497   4996 gb   4996 gb     1\n",
       "2498   4998 gb   4998 gb     1\n",
       "\n",
       "[2499 rows x 3 columns]"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_gb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_one</th>\n",
       "      <th>title_two</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 gb</td>\n",
       "      <td>4 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 gb</td>\n",
       "      <td>6 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 gb</td>\n",
       "      <td>8 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 gb</td>\n",
       "      <td>10 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 gb</td>\n",
       "      <td>12 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248497</th>\n",
       "      <td>998 gb</td>\n",
       "      <td>988 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248498</th>\n",
       "      <td>998 gb</td>\n",
       "      <td>990 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248499</th>\n",
       "      <td>998 gb</td>\n",
       "      <td>992 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248500</th>\n",
       "      <td>998 gb</td>\n",
       "      <td>994 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248501</th>\n",
       "      <td>998 gb</td>\n",
       "      <td>996 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248502 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       title_one title_two label\n",
       "0           2 gb      4 gb     0\n",
       "1           2 gb      6 gb     0\n",
       "2           2 gb      8 gb     0\n",
       "3           2 gb     10 gb     0\n",
       "4           2 gb     12 gb     0\n",
       "...          ...       ...   ...\n",
       "248497    998 gb    988 gb     0\n",
       "248498    998 gb    990 gb     0\n",
       "248499    998 gb    992 gb     0\n",
       "248500    998 gb    994 gb     0\n",
       "248501    998 gb    996 gb     0\n",
       "\n",
       "[248502 rows x 3 columns]"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_gb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gb_df = create_final_data(pos_gb_df, neg_gb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_one</th>\n",
       "      <th>title_two</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>3068 gb</td>\n",
       "      <td>3068 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>2470 gb</td>\n",
       "      <td>2470 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113556</th>\n",
       "      <td>458 gb</td>\n",
       "      <td>26 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>72 gb</td>\n",
       "      <td>72 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>4792 gb</td>\n",
       "      <td>4792 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20 gb</td>\n",
       "      <td>20 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76391</th>\n",
       "      <td>308 gb</td>\n",
       "      <td>398 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179368</th>\n",
       "      <td>722 gb</td>\n",
       "      <td>178 gb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>4688 gb</td>\n",
       "      <td>4688 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>4150 gb</td>\n",
       "      <td>4150 gb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4998 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       title_one title_two label\n",
       "1533     3068 gb   3068 gb     1\n",
       "1234     2470 gb   2470 gb     1\n",
       "113556    458 gb     26 gb     0\n",
       "35         72 gb     72 gb     1\n",
       "2395     4792 gb   4792 gb     1\n",
       "...          ...       ...   ...\n",
       "9          20 gb     20 gb     1\n",
       "76391     308 gb    398 gb     0\n",
       "179368    722 gb    178 gb     0\n",
       "2343     4688 gb   4688 gb     1\n",
       "2074     4150 gb   4150 gb     1\n",
       "\n",
       "[4998 rows x 3 columns]"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_gb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_gb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the numpy files of all the training embedddings\n",
    "We will have two numpy files:\n",
    "1. The training/validation/test sets\n",
    "2. The labels\n",
    "\"\"\"\n",
    "\n",
    "def create_embeddings(df):\n",
    "    # Create the numpy arrays for storing the embeddings and labels\n",
    "    total_embeddings = np.zeros(shape=(len(df), 2, MAX_LEN, EMBEDDING_SHAPE[0]))\n",
    "    labels = np.zeros(shape=(len(df)))\n",
    "    \n",
    "    # I know this is a terrible way of doing this, but iterate over the dataframe\n",
    "    # and generate the embeddings to add to the numpy array\n",
    "    for idx, row in enumerate(tqdm(df.itertuples())):\n",
    "        for word_idx, word in enumerate(row.title_one.split()):\n",
    "            total_embeddings[idx, 0, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        for word_idx, word in enumerate(row.title_two.split()):\n",
    "            total_embeddings[idx, 1, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        labels[idx] = row.label\n",
    "        \n",
    "    return total_embeddings, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(df, embeddings_name, labels_name):\n",
    "    \"\"\"\n",
    "    Saves the embeddings given the embeddings file name and labels file name\n",
    "    \"\"\"\n",
    "    if not os.path.exists('data/numpy_data/' + embeddings_name + '.npy'):\n",
    "        embeddings, labels = create_embeddings(df)\n",
    "        with open('data/numpy_data/' + embeddings_name + '.npy', 'wb') as f:\n",
    "            np.save(f, embeddings)\n",
    "\n",
    "        with open('data/numpy_data/' + labels_name + '.npy', 'wb') as f:\n",
    "            np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_labels(embeddings_name, labels_name):\n",
    "    loaded_embeddings = None\n",
    "    labels = None\n",
    "    with open('data/numpy_data/' + embeddings_name + '.npy', 'rb') as f:\n",
    "        loaded_embeddings = np.load(f)\n",
    "        loaded_embeddings = np.transpose(loaded_embeddings, (1, 0, 2, 3))\n",
    "    \n",
    "    with open('data/numpy_data/' + labels_name + '.npy', 'rb') as f:\n",
    "        labels = np.load(f)\n",
    "    \n",
    "    return loaded_embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Embeddings\n",
    "Save the embeddings for the different types of data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate everything\n",
    "total_data = pd.concat([final_computer_df, final_laptop_df, final_spec_df, final_gb_df, final_hard_drive_data, final_cpu_data, final_ram_data])\n",
    "total_data = total_data.sample(frac=1)\n",
    "MAX_LEN = get_max_len(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_array = {'a'}\n",
    "num_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iloc = total_data.iloc()\n",
    "for idx in range(len(total_data)):\n",
    "    row = total_iloc[idx]\n",
    "    titles = '{} {}'.format(row.title_one, row.title_two)\n",
    "    for char in titles:\n",
    "        char_array.update(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = []\n",
    "train_data2 = []\n",
    "labels = []\n",
    "total_iloc = total_data.iloc()\n",
    "for idx in range(len(total_data)):\n",
    "    title_one_base = [' '] * MAX_LEN\n",
    "    title_two_base = [' '] * MAX_LEN\n",
    "    row = total_iloc[idx]\n",
    "    \n",
    "    for row_idx, x in enumerate(row.title_one.split(' ')):\n",
    "        title_one_base[row_idx] = x\n",
    "    \n",
    "    for row_idx, x in enumerate(row.title_two.split(' ')):\n",
    "        title_two_base[row_idx] = x\n",
    "    \n",
    "    train_data1.append(title_one_base)\n",
    "    train_data2.append(title_two_base)\n",
    "    labels.append(row.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = np.asarray(train_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = np.asarray(train_data2)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['amd', 'ryzen', '3', ..., ' ', ' ', ' '],\n",
       "       ['22', 'gb', 'memory', ..., ' ', ' ', ' '],\n",
       "       ['buy', 'online', 'samsung', ..., ' ', ' ', ' '],\n",
       "       ...,\n",
       "       ['fp', 'reader', 'fast', ..., ' ', ' ', ' '],\n",
       "       ['4gb', 'ram', 'acer', ..., ' ', ' ', ' '],\n",
       "       ['amd', 'ryzen', '5', ..., ' ', ' ', ' ']], dtype='<U22')"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106236, 43)"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Embeddings\n",
    "save_embeddings(total_data, 'all_embeddings', 'all_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49108it [00:13, 3499.12it/s]"
     ]
    }
   ],
   "source": [
    "# Create Embeddings\n",
    "embeddings, labels = create_embeddings(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "embeddings, labels = load_embeddings_and_labels('all_embeddings', 'all_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35078"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (2, 102236, 43)\n",
      "Val shape: (2, 2000, 43)\n",
      "Test shape: (2, 2000, 43)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = train_data1[:len(labels) - 4000]\n",
    "X_train2 = train_data2[:len(labels) - 4000]\n",
    "X_train = np.stack((X_train1, X_train2))\n",
    "print('Training shape: ' + str(X_train.shape))\n",
    "\n",
    "X_val1 = train_data1[len(labels) - 4000:len(labels) - 2000]\n",
    "X_val2 = train_data2[len(labels) - 4000:len(labels) - 2000]\n",
    "X_val = np.stack((X_val1, X_val2))\n",
    "print('Val shape: ' + str(X_val.shape))\n",
    "\n",
    "\n",
    "X_test1 = train_data1[len(labels) - 2000:]\n",
    "X_test2 = train_data2[len(labels) - 2000:]\n",
    "X_test = np.stack((X_test1, X_test2))\n",
    "print('Test shape: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (102236,)\n",
      "Val shape: (2000,)\n",
      "Test shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "Y_train = labels[:len(labels) - 4000]\n",
    "print('Training labels shape:', str(Y_train.shape))\n",
    "\n",
    "Y_val = labels[len(labels) - 4000:len(labels) - 2000]\n",
    "print('Val shape:', str(Y_val.shape))\n",
    "\n",
    "Y_test = labels[len(labels) - 2000:]\n",
    "print('Test shape:', str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = convert_to_one_hot(Y_train.astype(np.int32), 2)\n",
    "Y_val = convert_to_one_hot(Y_val.astype(np.int32), 2)\n",
    "Y_test = convert_to_one_hot(Y_test.astype(np.int32), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Info\n",
    "\n",
    "For the model, we are going to use LSTMs with a Constrastive Loss Function \n",
    "that will also be used to predict whether the two products are the same \n",
    "\n",
    "First, we have to convert the titles to embeddings through FastText before feeding into the LSTM.\n",
    "The embedding part of this model will not be a layer because:\n",
    "* The fasttext model would be time consuming and annoying to get to work with an embedding layer in Keras\n",
    "* The fasttext model is not going to be getting its embeddings optimized, so there is really no point in adding it as an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embeddings(data):\n",
    "    embeddings = []\n",
    "    for row in data:\n",
    "        embeddings.append(np.array([fasttext_model[str(x.decode('utf-8'))] for x in row]))\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    return(embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return tf.square(x - y)\n",
    "\n",
    "def create_embeddings(vectors):\n",
    "    out = tf.numpy_function(func=to_embeddings, inp=[vectors], Tout='float32')\n",
    "    out.set_shape((None, MAX_LEN, EMBEDDING_SHAPE[0]))\n",
    "    return out\n",
    "\n",
    "def euclidean_dist_out_shape(shapes):\n",
    "    # Both inputs are fed in, so just use one of them and get the first value in the shape\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],)\n",
    "\n",
    "def siamese_network(input_shape):\n",
    "    # Defines our inputs\n",
    "    left_title = Input(input_shape, dtype='string')\n",
    "    right_title = Input(input_shape, dtype='string')\n",
    "    \n",
    "    # Create embeddings\n",
    "    CreateEmbeddings = Lambda(create_embeddings, output_shape=(None, MAX_LEN, EMBEDDING_SHAPE[0]))\n",
    "    left_embeddings = CreateEmbeddings(left_title)\n",
    "    right_embeddings = CreateEmbeddings(right_title)\n",
    "    \n",
    "    # The LSTM units\n",
    "    model = tf.keras.Sequential(name='siamese_model')\n",
    "    model.add(Bidirectional(LSTM(units=256, name='lstm_1')))\n",
    "    model.add(Dropout(rate=0.6))\n",
    "    \n",
    "    # The dense layers\n",
    "    model.add(Dense(units=512, activation='elu', name='dense_1'))\n",
    "    model.add(Dropout(rate=0.6))\n",
    "    model.add(Dense(units=256, activation='elu', name='dense_2'))\n",
    "    \n",
    "    # Forward propagate through the model to generate the encodings\n",
    "    encoded_left_title = model(left_embeddings)\n",
    "    encoded_right_title = model(right_embeddings)\n",
    "\n",
    "    SquareDistanceLayer = Lambda(square_distance)\n",
    "    distance = SquareDistanceLayer([encoded_left_title, encoded_right_title])\n",
    "    \n",
    "    prediction = Dense(units=2, activation='softmax')(distance)\n",
    "    # Create and return the network\n",
    "    siamese_net = tf.keras.Model(inputs=[left_title, right_title], outputs=prediction, name='siamese_network')\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for the constrastive loss, because 0 denotes that they are from the same class\n",
    "# and one denotes they are from a different class, I swaped the (Y) and (1 - Y) terms\n",
    "\n",
    "def constrastive_loss(y_true, y_pred):\n",
    "    margin = 2.0\n",
    "    d = y_pred\n",
    "    d_sqrt = tf.sqrt(d)\n",
    "    #tf.print('\\nY Pred: ', d, 'Shape: ', tf.shape(d))\n",
    "    #tf.print('\\nY True: ', y_true, 'Shape: ', tf.shape(y_true))\n",
    "    \n",
    "    loss = (y_true * d) + ((1 - y_true) * tf.square(tf.maximum(0., margin - d_sqrt)))\n",
    "    \n",
    "    #tf.print('\\n Constrastive Loss: ', loss, 'Shape: ', tf.shape(loss))\n",
    "    loss = 0.5 * tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric for constrastive loss because values close to 0 are equal and values high are different\n",
    "# 0.5 is the threshold here\n",
    "def constrastive_accuracy(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_true, tf.cast(y_pred < 0.5, y_true.dtype)), y_true.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    \"\"\"\n",
    "    Saves a model with a particular name\n",
    "    \"\"\"\n",
    "    model.save('models/' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese_network\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_236 (InputLayer)          [(None, 43)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_237 (InputLayer)          [(None, 43)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_126 (Lambda)             (None, 43, 300)      0           input_236[0][0]                  \n",
      "                                                                 input_237[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "siamese_model (Sequential)      (None, 256)          1534720     lambda_126[0][0]                 \n",
      "                                                                 lambda_126[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_127 (Lambda)             (None, 256)          0           siamese_model[0][0]              \n",
      "                                                                 siamese_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 2)            514         lambda_127[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,535,234\n",
      "Trainable params: 1,535,234\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = siamese_network(MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 10/799 [..............................] - ETA: 18:37 - loss: 0.6947 - accuracy: 0.4945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-748-2c750961f17d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=[X_train1, X_train2], y=Y_train, batch_size=128, epochs=30, validation_data=([X_val[0], X_val[1]], Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.3439 - accuracy: 0.8830\n",
      "test loss, test acc:  [0.343930117726326, 0.883]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "results = model.evaluate([X_test1, X_test2], Y_test, batch_size=16)\n",
    "print('test loss, test acc: ', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's name\n",
    "model_name = '0.1.2_Softmax-LSTM-128_batch_80_epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing\n",
    "Converts titles into embeddings arrays and allow the model to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_one = '2020 Dell XPS 13.3\" FHD Laptop Computer, Intel Core i5-10210U Processor, 8GB RAM, 512GB PCIe SSD, Baklit Keyboard, MaxxAudio, HD Webcam, Win 10, Platinum Silver, 32GB Snow Bell USB Card'\n",
    "title_two = '2020 Dell XPS 13.3\" Ultrabook, Intel Core i5-10230U, HD Webcam, (16GB RAM, 512GB PCIe SSD) Backlit, Windows 10, Silver'\n",
    "title_one = '16 GB'\n",
    "title_two = '64 gb'\n",
    "title_one_arr = np.zeros((1, MAX_LEN, 300))\n",
    "title_two_arr = np.zeros((1, MAX_LEN, 300))\n",
    "title_one = remove_stop_words(title_one.lower())\n",
    "title_two = remove_stop_words(title_two.lower())\n",
    "\n",
    "for idx, word in enumerate(title_one.split(' ')):\n",
    "    title_one_arr[0, idx] = fasttext_model[word]\n",
    "    \n",
    "for idx, word in enumerate(title_two.split(' ')):\n",
    "    title_two_arr[0, idx] = fasttext_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5059981 , 0.49400198]], dtype=float32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([title_one_arr, title_two_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "chars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', ' ']\n",
    "\n",
    "holy = dict(zip(chars, nums))\n",
    "holier = dict(zip(nums, chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 300])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.map_fn(fn = map_function, elems=tf.constant([['jsdlfj', 'asdfasdf', 'sdfasdf'], ['sdfasdf', 'sdfasdf', 'sdfasdf']]), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=string, numpy=\n",
       "array([[b'sjdf', b'sdf'],\n",
       "       [b'sdfaf', b'sdfa']], dtype=object)>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([['sjdf', 'sdf'], ['sdfaf', 'sdfa']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
