{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Lambda, Concatenate\n",
    "\n",
    "# Have to download the stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fasttext model (we are using the largest one they offer [600B tokens])\n",
    "fasttext_model = fasttext.load_model('models/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processsing and Organization\n",
    "Here, all we really want to do is prepare the data for training. This is **only** the data from **Gold Standard** This includes:\n",
    "* Simplifying the original data\n",
    "* Normalizing the data \n",
    "* Balancing the positive and negative examples\n",
    "* Creating the embedding representations that will actually get fed into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(phrase):\n",
    "    # Creates the stopwords\n",
    "    to_stop = stopwords.words('english')\n",
    "    punctuation = \"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~ \"\n",
    "    for c in punctuation:\n",
    "        to_stop.append(c)\n",
    "\n",
    "    to_stop.append('null')\n",
    "    \n",
    "    for punc in punctuation:\n",
    "        phrase = phrase.replace(punc, ' ')\n",
    "    \n",
    "    return ' '.join((' '.join([x for x in phrase.split(' ') if x not in to_stop])).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing and normalizing the data\n",
    "\"\"\"\n",
    "Essentially, we want to only have three attributes for each training example: title_one, title_two, label\n",
    "For normalization, we are just going to use the nltk stopwords and punctuation\n",
    "\"\"\"\n",
    "\n",
    "def preprocessing(orig_data):\n",
    "    \"\"\"\n",
    "    Normalizes the data by getting rid of stopwords and punctuation\n",
    "    \"\"\"\n",
    "    \n",
    "    # The new names of the columns\n",
    "    column_names = ['title_one', 'title_two', 'label']\n",
    "    # A new dataframe for the data we are going to be creating\n",
    "    norm_computers = pd.DataFrame(columns = column_names)\n",
    "    # Iterate over the original dataframe (I know it is slow and there are probably better ways to do it)\n",
    "    for row in orig_data.itertuples():\n",
    "        title_left = remove_stop_words(row.title_left)\n",
    "        title_right = remove_stop_words(row.title_right)\n",
    "        \n",
    "        # Append the newly created row (title_left, title_right, label) to the new dataframe\n",
    "        norm_computers = norm_computers.append(pd.DataFrame([[title_left, title_right, row.label]], columns=column_names))\n",
    "        \n",
    "    return norm_computers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_data():\n",
    "    \"\"\"\n",
    "    Creates and saves a simpler version of the original data that only contains the the two titles and the label.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataset of computer parts\n",
    "    computers_df = pd.read_json('data/computers_train/computers_train_xlarge_normalized.json.gz',compression='gzip', lines=True)\n",
    "    norm_computers = preprocessing(computers_df)\n",
    "    \n",
    "    # Save the new normalized and simplified data to a CSV file to load later\n",
    "    norm_computers.to_csv('data/computers_train/computers_train_xlarge_norm_simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the data if the simple and normalized data does not exist\n",
    "if not os.path.exists('data/computers_train/computers_train_xlarge_norm_simple.csv'):\n",
    "    create_simple_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "computer_df = pd.read_csv('data/computers_train/computers_train_xlarge_norm_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See some of the data. There is clearly a separation between the positive and negative examples\n",
    "computer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_df(df):\n",
    "    \"\"\"\n",
    "    Returns a shuffled dataframe with an equal amount of positive and negative examples\n",
    "    \"\"\"\n",
    "    # Get the positive and negative examples\n",
    "    pos_df = df.loc[df['label'] == 1]\n",
    "    neg_df = df.loc[df['label'] == 0]\n",
    "    \n",
    "    # Shuffle the data\n",
    "    pos_df = pos_df.sample(frac=1)\n",
    "    neg_df = neg_df.sample(frac=1)\n",
    "    \n",
    "    # Concatenate the positive and negative examples and \n",
    "    # make sure there are only as many negative examples as positive examples\n",
    "    final_df = pd.concat([pos_df, neg_df[:len(pos_df)]])\n",
    "    \n",
    "    # Shuffle the final data once again\n",
    "    final_df.sample(frac=1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the dataframe with equal numbers of positive and negative examples\n",
    "# and is shuffled\n",
    "if not os.path.exists('data/computers_train/computers_train_bal_shuffle.csv'):\n",
    "    create_train_df(computer_df).to_csv('data/computers_train/computers_train_bal_shuffle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/computers_train/computers_train_bal_shuffle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_one</th>\n",
       "      <th>title_two</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corsair carbide air 240 windowed</td>\n",
       "      <td>corsair carbide series air 240 cube micro atx ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a8 7670k black edition quad core amd cpu fan h...</td>\n",
       "      <td>amd a8 7650k 3 3ghz pccomponentes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazonbasics 13 3 inch laptop sleeve black acc...</td>\n",
       "      <td>amazonbasics 13 3 inch laptop sleeve black car...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eg0146fartr hp 146 gb 6g 10k 2 5 dp sas hdd ne...</td>\n",
       "      <td>eg0146fartr hp 146 gb 6g 10k 2 5 dp sas hdd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usb 3 0 external adapter cable 2 5 inch hard d...</td>\n",
       "      <td>transcend ssd370 solid state drive ssd 2 5 sat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19375</th>\n",
       "      <td>356816 001 ml350t g4p xeon 3 2 2mb 512mb whole...</td>\n",
       "      <td>409159 b21 hp xeon e5345 2 33ghz dl160 g3 new ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19376</th>\n",
       "      <td>buy online samsung 750 evo series 120gb ssd mz...</td>\n",
       "      <td>ssd 750 basic 120 gb tradineur com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19377</th>\n",
       "      <td>628061 s21 hp g8 g9 3 tb 6g 7 2k 5 sata sc new...</td>\n",
       "      <td>628061 s21 hp g8 g9 3 tb 6g 7 2k 5 sata sc new...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19378</th>\n",
       "      <td>buy online zotac gtx 1060 6gb amp edition grap...</td>\n",
       "      <td>msi nvidia geforce gtx 1080 8gb gaming x rgb g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19379</th>\n",
       "      <td>hyperx fury blue 4gb 1600mhz ddr3 tradineur com</td>\n",
       "      <td>mem ria ram hyperx fury 4gb 1x4gb ddr3 1600mhz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19380 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title_one  \\\n",
       "0                       corsair carbide air 240 windowed   \n",
       "1      a8 7670k black edition quad core amd cpu fan h...   \n",
       "2      amazonbasics 13 3 inch laptop sleeve black acc...   \n",
       "3      eg0146fartr hp 146 gb 6g 10k 2 5 dp sas hdd ne...   \n",
       "4      usb 3 0 external adapter cable 2 5 inch hard d...   \n",
       "...                                                  ...   \n",
       "19375  356816 001 ml350t g4p xeon 3 2 2mb 512mb whole...   \n",
       "19376  buy online samsung 750 evo series 120gb ssd mz...   \n",
       "19377  628061 s21 hp g8 g9 3 tb 6g 7 2k 5 sata sc new...   \n",
       "19378  buy online zotac gtx 1060 6gb amp edition grap...   \n",
       "19379    hyperx fury blue 4gb 1600mhz ddr3 tradineur com   \n",
       "\n",
       "                                               title_two  label  \n",
       "0      corsair carbide series air 240 cube micro atx ...      1  \n",
       "1                      amd a8 7650k 3 3ghz pccomponentes      1  \n",
       "2      amazonbasics 13 3 inch laptop sleeve black car...      1  \n",
       "3            eg0146fartr hp 146 gb 6g 10k 2 5 dp sas hdd      1  \n",
       "4      transcend ssd370 solid state drive ssd 2 5 sat...      0  \n",
       "...                                                  ...    ...  \n",
       "19375  409159 b21 hp xeon e5345 2 33ghz dl160 g3 new ...      0  \n",
       "19376                 ssd 750 basic 120 gb tradineur com      1  \n",
       "19377  628061 s21 hp g8 g9 3 tb 6g 7 2k 5 sata sc new...      1  \n",
       "19378  msi nvidia geforce gtx 1080 8gb gaming x rgb g...      0  \n",
       "19379  mem ria ram hyperx fury 4gb 1x4gb ddr3 1600mhz...      1  \n",
       "\n",
       "[19380 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laptop Data Preprocessing\n",
    "* Normalize the data\n",
    "* Create negative examples that represent when only a couple of attributes of the laptop data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the laptop data\n",
    "laptop_df = pd.read_csv('data/computers_train/laptops.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price_euros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 2.3GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 640</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>1339.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Macbook Air</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1440x900</td>\n",
       "      <td>Intel Core i5 1.8GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics 6000</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>898.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HP</td>\n",
       "      <td>250 G6</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Full HD 1920x1080</td>\n",
       "      <td>Intel Core i5 7200U 2.5GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel HD Graphics 620</td>\n",
       "      <td>No OS</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>575.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>15.4</td>\n",
       "      <td>IPS Panel Retina Display 2880x1800</td>\n",
       "      <td>Intel Core i7 2.7GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>AMD Radeon Pro 455</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>2537.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Apple</td>\n",
       "      <td>MacBook Pro</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 3.1GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 650</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>1803.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1316</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Yoga 500-14ISK</td>\n",
       "      <td>2 in 1 Convertible</td>\n",
       "      <td>14.0</td>\n",
       "      <td>IPS Panel Full HD / Touchscreen 1920x1080</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>4GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel HD Graphics 520</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.8kg</td>\n",
       "      <td>638.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1317</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Yoga 900-13ISK</td>\n",
       "      <td>2 in 1 Convertible</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Quad HD+ / Touchscreen 3200x1800</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>Intel HD Graphics 520</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.3kg</td>\n",
       "      <td>1499.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>1318</td>\n",
       "      <td>Lenovo</td>\n",
       "      <td>IdeaPad 100S-14IBR</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Celeron Dual Core N3050 1.6GHz</td>\n",
       "      <td>2GB</td>\n",
       "      <td>64GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>1.5kg</td>\n",
       "      <td>229.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>1319</td>\n",
       "      <td>HP</td>\n",
       "      <td>15-AC110nv (i7-6500U/6GB/1TB/Radeon</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Core i7 6500U 2.5GHz</td>\n",
       "      <td>6GB</td>\n",
       "      <td>1TB HDD</td>\n",
       "      <td>AMD Radeon R5 M330</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>2.19kg</td>\n",
       "      <td>764.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>1320</td>\n",
       "      <td>Asus</td>\n",
       "      <td>X553SA-XX031T (N3050/4GB/500GB/W10)</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1366x768</td>\n",
       "      <td>Intel Celeron Dual Core N3050 1.6GHz</td>\n",
       "      <td>4GB</td>\n",
       "      <td>500GB HDD</td>\n",
       "      <td>Intel HD Graphics</td>\n",
       "      <td>Windows 10</td>\n",
       "      <td>2.2kg</td>\n",
       "      <td>369.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 Company                              Product  \\\n",
       "0              1   Apple                          MacBook Pro   \n",
       "1              2   Apple                          Macbook Air   \n",
       "2              3      HP                               250 G6   \n",
       "3              4   Apple                          MacBook Pro   \n",
       "4              5   Apple                          MacBook Pro   \n",
       "...          ...     ...                                  ...   \n",
       "1298        1316  Lenovo                       Yoga 500-14ISK   \n",
       "1299        1317  Lenovo                       Yoga 900-13ISK   \n",
       "1300        1318  Lenovo                   IdeaPad 100S-14IBR   \n",
       "1301        1319      HP  15-AC110nv (i7-6500U/6GB/1TB/Radeon   \n",
       "1302        1320    Asus  X553SA-XX031T (N3050/4GB/500GB/W10)   \n",
       "\n",
       "                TypeName  Inches                            ScreenResolution  \\\n",
       "0              Ultrabook    13.3          IPS Panel Retina Display 2560x1600   \n",
       "1              Ultrabook    13.3                                    1440x900   \n",
       "2               Notebook    15.6                           Full HD 1920x1080   \n",
       "3              Ultrabook    15.4          IPS Panel Retina Display 2880x1800   \n",
       "4              Ultrabook    13.3          IPS Panel Retina Display 2560x1600   \n",
       "...                  ...     ...                                         ...   \n",
       "1298  2 in 1 Convertible    14.0   IPS Panel Full HD / Touchscreen 1920x1080   \n",
       "1299  2 in 1 Convertible    13.3  IPS Panel Quad HD+ / Touchscreen 3200x1800   \n",
       "1300            Notebook    14.0                                    1366x768   \n",
       "1301            Notebook    15.6                                    1366x768   \n",
       "1302            Notebook    15.6                                    1366x768   \n",
       "\n",
       "                                       Cpu   Ram               Memory  \\\n",
       "0                     Intel Core i5 2.3GHz   8GB            128GB SSD   \n",
       "1                     Intel Core i5 1.8GHz   8GB  128GB Flash Storage   \n",
       "2               Intel Core i5 7200U 2.5GHz   8GB            256GB SSD   \n",
       "3                     Intel Core i7 2.7GHz  16GB            512GB SSD   \n",
       "4                     Intel Core i5 3.1GHz   8GB            256GB SSD   \n",
       "...                                    ...   ...                  ...   \n",
       "1298            Intel Core i7 6500U 2.5GHz   4GB            128GB SSD   \n",
       "1299            Intel Core i7 6500U 2.5GHz  16GB            512GB SSD   \n",
       "1300  Intel Celeron Dual Core N3050 1.6GHz   2GB   64GB Flash Storage   \n",
       "1301            Intel Core i7 6500U 2.5GHz   6GB              1TB HDD   \n",
       "1302  Intel Celeron Dual Core N3050 1.6GHz   4GB            500GB HDD   \n",
       "\n",
       "                               Gpu       OpSys  Weight  Price_euros  \n",
       "0     Intel Iris Plus Graphics 640       macOS  1.37kg      1339.69  \n",
       "1           Intel HD Graphics 6000       macOS  1.34kg       898.94  \n",
       "2            Intel HD Graphics 620       No OS  1.86kg       575.00  \n",
       "3               AMD Radeon Pro 455       macOS  1.83kg      2537.45  \n",
       "4     Intel Iris Plus Graphics 650       macOS  1.37kg      1803.60  \n",
       "...                            ...         ...     ...          ...  \n",
       "1298         Intel HD Graphics 520  Windows 10   1.8kg       638.00  \n",
       "1299         Intel HD Graphics 520  Windows 10   1.3kg      1499.00  \n",
       "1300             Intel HD Graphics  Windows 10   1.5kg       229.00  \n",
       "1301            AMD Radeon R5 M330  Windows 10  2.19kg       764.00  \n",
       "1302             Intel HD Graphics  Windows 10   2.2kg       369.00  \n",
       "\n",
       "[1303 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Creation\n",
    "Generates the embeddings and saves them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LEN: 42 EMBEDDING_SHAPE: (300,) m: 19380\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Definitions of some sizes in the training set\n",
    "\"\"\"\n",
    "MAX_LEN = 42\n",
    "EMBEDDING_SHAPE = (300,)\n",
    "m = 19380\n",
    "print('MAX_LEN: ' + str(MAX_LEN), 'EMBEDDING_SHAPE: ' + str(EMBEDDING_SHAPE), 'm: ' + str(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the numpy files of all the training embedddings\n",
    "We will have two numpy files:\n",
    "1. The training/validation/test sets\n",
    "2. The labels\n",
    "\"\"\"\n",
    "\n",
    "def create_embeddings(df):\n",
    "    # Create the numpy arrays for storing the embeddings and labels\n",
    "    total_embeddings = np.zeros(shape=(m, 2, MAX_LEN, EMBEDDING_SHAPE[0]))\n",
    "    labels = np.zeros(shape=(m))\n",
    "    \n",
    "    # I know this is a terrible way of doing this, but iterate over the dataframe\n",
    "    # and generate the embeddings to add to the numpy array\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        for word_idx, word in enumerate(row.title_one.split()):\n",
    "            total_embeddings[idx, 0, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        for word_idx, word in enumerate(row.title_two.split()):\n",
    "            total_embeddings[idx, 1, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        labels[idx] = row.label\n",
    "        \n",
    "    return total_embeddings, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(df, embeddings_name, labels_name):\n",
    "    \"\"\"\n",
    "    Saves the embeddings given the embeddings file name and labels file name\n",
    "    \"\"\"\n",
    "    if not os.path.exists('data/computers_numpy/' + embeddings_name + '.npy'):\n",
    "        embeddings, labels = create_embeddings(df)\n",
    "        with open('data/computers_numpy/' + embeddings_name + '.npy', 'wb') as f:\n",
    "            np.save(f, embeddings)\n",
    "\n",
    "        with open('data/computers_numpy/' + labels_name + '.npy', 'wb') as f:\n",
    "            np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_labels(embeddings_name, labels_name):\n",
    "    loaded_embeddings = None\n",
    "    labels = None\n",
    "    with open('data/computers_numpy/' + embeddings_name + '.npy', 'rb') as f:\n",
    "        loaded_embeddings = np.load(f)\n",
    "        loaded_embeddings = np.transpose(loaded_embeddings, (1, 0, 2, 3))\n",
    "    \n",
    "    with open('data/computers_numpy/' + labels_name + '.npy', 'rb') as f:\n",
    "        labels = np.load(f)\n",
    "    \n",
    "    return loaded_embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(df):\n",
    "    max_len = 0\n",
    "    for row in df.itertuples():\n",
    "        if len(row.title_one.split(' ')) > max_len:\n",
    "            max_len = len(row.title_one.split(' '))\n",
    "            \n",
    "        if len(row.title_two.split(' ')) > max_len:\n",
    "            max_len = len(row.title_two.split(' '))\n",
    "    \n",
    "    return max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(df, 'bal_embeddings', 'bal_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, labels = load_embeddings_and_labels('bal_embeddings', 'bal_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (2, 15000, 42, 300)\n",
      "Val shape: (2, 2000, 42, 300)\n",
      "Test shape: (2, 2380, 42, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = embeddings[0, :15000]\n",
    "X_train2 = embeddings[1, :15000]\n",
    "X_train = np.stack((X_train1, X_train2))\n",
    "print('Training shape: ' + str(X_train.shape))\n",
    "\n",
    "X_val1 = embeddings[0, 15000:17000]\n",
    "X_val2 = embeddings[1, 15000:17000]\n",
    "X_val = np.stack((X_val1, X_val2))\n",
    "print('Val shape: ' + str(X_val.shape))\n",
    "\n",
    "X_test1 = embeddings[0, 17000:]\n",
    "X_test2 = embeddings[1, 17000:]\n",
    "X_test = np.stack((X_test1, X_test2))\n",
    "print('Test shape: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = labels[:15000]\n",
    "Y_val = labels[15000:17000]\n",
    "Y_test = labels[17000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = convert_to_one_hot(Y_train.astype(np.int32), 2)\n",
    "Y_val = convert_to_one_hot(Y_val.astype(np.int32), 2)\n",
    "Y_test = convert_to_one_hot(Y_test.astype(np.int32), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Info\n",
    "\n",
    "For the model, we are going to use LSTMs with a Constrastive Loss Function \n",
    "that will also be used to predict whether the two products are the same \n",
    "\n",
    "First, we have to convert the titles to embeddings through FastText before feeding into the LSTM.\n",
    "The embedding part of this model will not be a layer because:\n",
    "* The fasttext model would be time consuming and annoying to get to work with an embedding layer in Keras\n",
    "* The fasttext model is not going to be getting its embeddings optimized, so there is really no point in adding it as an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return tf.square(x - y)\n",
    "\n",
    "def euclidean_dist_out_shape(shapes):\n",
    "    # Both inputs are fed in, so just use one of them and get the first value in the shape\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],)\n",
    "\n",
    "def siamese_network(input_shape):\n",
    "    # Defines our inputs\n",
    "    left_title = Input(input_shape, dtype='float32')\n",
    "    right_title = Input(input_shape, dtype='float32')\n",
    "    \n",
    "    # The LSTM units\n",
    "    model = tf.keras.Sequential(name='siamese_model')\n",
    "    model.add(LSTM(units=256, return_sequences=True, name='lstm_1'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(LSTM(units=128, return_sequences=True, name='lstm_2'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(LSTM(units=128, name='lstm_3'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    \n",
    "    # The dense layers\n",
    "    model.add(Dense(units=1024, activation='elu', name='dense_1'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(units=512, activation='elu', name='dense_2'))\n",
    "    \n",
    "    # Forward propagate through the model to generate the encodings\n",
    "    encoded_left_title = model(left_title)\n",
    "    encoded_right_title = model(right_title)\n",
    "\n",
    "    SquareDistanceLayer = Lambda(square_distance)\n",
    "    distance = SquareDistanceLayer([encoded_left_title, encoded_right_title])\n",
    "    \n",
    "    prediction = Dense(units=2, activation='softmax')(distance)\n",
    "    # Create and return the network\n",
    "    siamese_net = tf.keras.Model(inputs=[left_title, right_title], outputs=prediction, name='siamese_network')\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for the constrastive loss, because 0 denotes that they are from the same class\n",
    "# and one denotes they are from a different class, I swaped the (Y) and (1 - Y) terms\n",
    "\n",
    "def constrastive_loss(y_true, y_pred):\n",
    "    margin = 2.0\n",
    "    d = y_pred\n",
    "    d_sqrt = tf.sqrt(d)\n",
    "    #tf.print('\\nY Pred: ', d, 'Shape: ', tf.shape(d))\n",
    "    #tf.print('\\nY True: ', y_true, 'Shape: ', tf.shape(y_true))\n",
    "    \n",
    "    loss = (y_true * d) + ((1 - y_true) * tf.square(tf.maximum(0., margin - d_sqrt)))\n",
    "    \n",
    "    #tf.print('\\n Constrastive Loss: ', loss, 'Shape: ', tf.shape(loss))\n",
    "    loss = 0.5 * tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric for constrastive loss because values close to 0 are equal and values high are different\n",
    "# 0.5 is the threshold here\n",
    "def constrastive_accuracy(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_true, tf.cast(y_pred < 0.5, y_true.dtype)), y_true.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    \"\"\"\n",
    "    Saves a model with a particular name\n",
    "    \"\"\"\n",
    "    model.save('models/' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese_network\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 42, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 42, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "siamese_model (Sequential)      (None, 512)          1555968     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 512)          0           siamese_model[0][0]              \n",
      "                                                                 siamese_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            1026        lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,556,994\n",
      "Trainable params: 1,556,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = siamese_network((MAX_LEN, EMBEDDING_SHAPE[0],))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "lr = 0.001\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 2000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.5060 - accuracy: 0.7527 - val_loss: 0.5491 - val_accuracy: 0.7360\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 223s 15ms/sample - loss: 0.4775 - accuracy: 0.7695 - val_loss: 0.6285 - val_accuracy: 0.7260\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.4826 - accuracy: 0.7759 - val_loss: 0.5436 - val_accuracy: 0.7605\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 223s 15ms/sample - loss: 0.4612 - accuracy: 0.7825 - val_loss: 0.5033 - val_accuracy: 0.7790\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 223s 15ms/sample - loss: 0.4378 - accuracy: 0.7966 - val_loss: 0.4869 - val_accuracy: 0.8060\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 223s 15ms/sample - loss: 0.4191 - accuracy: 0.8051 - val_loss: 0.4793 - val_accuracy: 0.7885\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.4148 - accuracy: 0.8097 - val_loss: 0.4714 - val_accuracy: 0.7910\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.3933 - accuracy: 0.8235 - val_loss: 0.4724 - val_accuracy: 0.7890\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.3910 - accuracy: 0.8265 - val_loss: 0.4898 - val_accuracy: 0.7870\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.3768 - accuracy: 0.8328 - val_loss: 0.4843 - val_accuracy: 0.7835\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.3656 - accuracy: 0.8363 - val_loss: 0.4990 - val_accuracy: 0.7890\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 223s 15ms/sample - loss: 0.3519 - accuracy: 0.8468 - val_loss: 0.4773 - val_accuracy: 0.7905\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.3462 - accuracy: 0.8524 - val_loss: 0.6410 - val_accuracy: 0.7620\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.3355 - accuracy: 0.8592 - val_loss: 0.4516 - val_accuracy: 0.8090\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.3274 - accuracy: 0.8620 - val_loss: 0.5073 - val_accuracy: 0.7875\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.3151 - accuracy: 0.8675 - val_loss: 0.6584 - val_accuracy: 0.7495\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 223s 15ms/sample - loss: 0.3161 - accuracy: 0.8675 - val_loss: 0.4756 - val_accuracy: 0.8060\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.3070 - accuracy: 0.8717 - val_loss: 0.4789 - val_accuracy: 0.8020\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2933 - accuracy: 0.8762 - val_loss: 0.4217 - val_accuracy: 0.8505\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2894 - accuracy: 0.8821 - val_loss: 0.4051 - val_accuracy: 0.8550\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2795 - accuracy: 0.8873 - val_loss: 0.4262 - val_accuracy: 0.8215\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2731 - accuracy: 0.8895 - val_loss: 0.4038 - val_accuracy: 0.8510\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2676 - accuracy: 0.8942 - val_loss: 0.8293 - val_accuracy: 0.7670\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 227s 15ms/sample - loss: 0.2656 - accuracy: 0.8954 - val_loss: 0.4090 - val_accuracy: 0.8430\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2608 - accuracy: 0.8965 - val_loss: 0.5973 - val_accuracy: 0.7895\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2661 - accuracy: 0.8941 - val_loss: 0.4199 - val_accuracy: 0.8545\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2511 - accuracy: 0.9022 - val_loss: 0.3932 - val_accuracy: 0.8555\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2486 - accuracy: 0.9023 - val_loss: 0.3727 - val_accuracy: 0.8790\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2423 - accuracy: 0.9055 - val_loss: 0.4353 - val_accuracy: 0.8450\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2362 - accuracy: 0.9086 - val_loss: 0.4108 - val_accuracy: 0.8520\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 226s 15ms/sample - loss: 0.2396 - accuracy: 0.9061 - val_loss: 0.7739 - val_accuracy: 0.7820\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 227s 15ms/sample - loss: 0.2363 - accuracy: 0.9107 - val_loss: 0.3441 - val_accuracy: 0.8885\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2227 - accuracy: 0.9139 - val_loss: 0.4195 - val_accuracy: 0.8410\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2219 - accuracy: 0.9151 - val_loss: 0.3643 - val_accuracy: 0.8680\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2173 - accuracy: 0.9147 - val_loss: 0.3440 - val_accuracy: 0.8980\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2199 - accuracy: 0.9137 - val_loss: 0.3631 - val_accuracy: 0.8730\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2068 - accuracy: 0.9220 - val_loss: 0.3669 - val_accuracy: 0.8760\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 224s 15ms/sample - loss: 0.2126 - accuracy: 0.9230 - val_loss: 0.3264 - val_accuracy: 0.9025\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.1978 - accuracy: 0.9236 - val_loss: 0.5309 - val_accuracy: 0.8230\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 225s 15ms/sample - loss: 0.2003 - accuracy: 0.9221 - val_loss: 0.3769 - val_accuracy: 0.8765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d34eb2a188>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x=[X_train1, X_train2], y=Y_train, batch_size=64, epochs=50, validation_data=([X_val[0], X_val[1]], Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "results = model.evaluate([X_test1, X_test2], Y_test, batch_size=16)\n",
    "print('test loss, test acc: ', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jason\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_name = 'Softmax-LSTM-_epochs_loss'\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing\n",
    "Converts titles into embeddings arrays and allow the model to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_one = 'True Wireless Earbuds VANKYO X200 Bluetooth 5 0 Earbuds in Ear TWS Stereo Headphones Smart LED Display Charging Case IPX8 Waterproof 120H Playtime Built Mic Deep Bass Sports Work'\n",
    "title_two = 'TOZO T10 Bluetooth 5 0 Wireless Earbuds Wireless Charging Case IPX8 Waterproof TWS Stereo Headphones Ear Built Mic Headset Premium Sound Deep Bass Sport Black'\n",
    "title_one_arr = np.zeros((1, 42, 300))\n",
    "title_two_arr = np.zeros((1, 42, 300))\n",
    "title_one.lower()\n",
    "title_two.lower()\n",
    "for idx, word in enumerate(title_one.split(' ')):\n",
    "    title_one_arr[0, idx] = fasttext_model[word]\n",
    "    \n",
    "for idx, word in enumerate(title_two.split(' ')):\n",
    "    title_two_arr[0, idx] = fasttext_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27092224, 0.7290778 ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([title_one_arr, title_two_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
